<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R语言笔记 on 行行重行行</title>
    <link>http://youngspring1.github.io/categories/r%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in R语言笔记 on 行行重行行</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2016. All rights reserved.</copyright>
    <lastBuildDate>Sat, 14 May 2016 23:19:28 +0800</lastBuildDate>
    <atom:link href="http://youngspring1.github.io/categories/r%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>R语言笔记05</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-R05/</link>
      <pubDate>Sat, 14 May 2016 23:19:28 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-R05/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第五单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;text-analytics:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;Text Analytics&lt;/h2&gt;

&lt;p&gt;第三单元的主题是文本分析。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;指数回归:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;指数回归&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记04</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-R04/</link>
      <pubDate>Tue, 10 May 2016 19:44:08 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-R04/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第四单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;trees:0257109cd77173fde404dfe977de0c33&#34;&gt;Trees&lt;/h2&gt;

&lt;p&gt;第四单元的主题是决策树。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:0257109cd77173fde404dfe977de0c33&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;cart-classification-and-regression-trees:0257109cd77173fde404dfe977de0c33&#34;&gt;CART(classification and regression trees)&lt;/h4&gt;

&lt;h6 id=&#34;决策树:0257109cd77173fde404dfe977de0c33&#34;&gt;决策树&lt;/h6&gt;

&lt;p&gt;自变量是决策树上的节点(splits)。但是注意，不是每个自变量都有一个节点；也就是说，有的自变量有多个节点(随着取值的不同，导致因变量的结果也不同)，有的自变量没有节点(对因变量影响很小)。&lt;br /&gt;
因变量是决策树上的叶子/终端(leaves/nodes)。此图上的因变量的取值是0或者1。&lt;br /&gt;
在各个节点，根据各个自变量的取值，最终到达叶子节点，也就得到了因变量的取值。&lt;br /&gt;
注意，决策树的左边，节点的判断语句总是为True／Yes，右边节点的判断语句总是为False／No。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160517-tree-Rplot.png&#34; alt=&#34;tree&#34; /&gt;&lt;br /&gt;
最左的分支表示，如果 LowerCou=lbr 且 Responde=CRI 且 Petition=CIT，那么因变量的取值为0。&lt;br /&gt;
最右的分支表示，如果 LowerCou!=lbr 且 Responde=STA，那么因变量的取值为1。&lt;/p&gt;

&lt;h6 id=&#34;决策树的大小:0257109cd77173fde404dfe977de0c33&#34;&gt;决策树的大小&lt;/h6&gt;

&lt;p&gt;minbucket可以理解为，决策树被节点分割后，每个bucket数据的数量。&lt;br /&gt;
minbucket越大，分组越少，split越少。&lt;br /&gt;
minbucket越小，分组越多，split越多。&lt;/p&gt;

&lt;h6 id=&#34;classification-tree-和-regression-tree:0257109cd77173fde404dfe977de0c33&#34;&gt;Classification tree 和 Regression tree&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;Classification tree analysis is when the predicted outcome is the class to which the data belongs.（简单的讲，预测值是0和1，比如支持还是反对）&lt;/li&gt;
&lt;li&gt;Regression tree analysis is when the predicted outcome can be considered a real number.（简单的讲，预测值是可变的，比如房价等等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;体现在代码中的话，
如果指定了type = &amp;ldquo;class&amp;rdquo;，那么是 Classification tree。&lt;br /&gt;
如果没有指定type = &amp;ldquo;class&amp;rdquo;，那么是 Regression tree。&lt;/p&gt;

&lt;h4 id=&#34;random-forest:0257109cd77173fde404dfe977de0c33&#34;&gt;Random Forest&lt;/h4&gt;

&lt;p&gt;随机森林，被设计出来用于提高CART的精度。&lt;br /&gt;
和字面意思类似，如果决策树只有一棵树，那么随机森林会创建多个决策树，然后找到效果最好的那一个。&lt;br /&gt;
那么它是如何创建多个决策树的呢，有点复杂。&lt;br /&gt;
它并不是多次调用rpart()，简单的调整几个参数而已。&lt;br /&gt;
每个决策树所用的数据，都只是原数据的随机subset或者说随机子集。&lt;br /&gt;
如果训练集被分成1，2，3，4，5 这五个子集，那么第一次可能选取2，4，5，2，1，第二次可能选取3，5，1，5，2。&lt;/p&gt;

&lt;p&gt;参数nodesize&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;类似于minbucket，每个子集的最小数目。它越小，生成的决策树越大。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数ntree&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;生成多少个决策树。一般几百个就够了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好消息是，参数的选取，相比CART而言，对结果的影响没有那么大。&lt;/p&gt;

&lt;h4 id=&#34;cross-validation:0257109cd77173fde404dfe977de0c33&#34;&gt;Cross Validation&lt;/h4&gt;

&lt;p&gt;minbucket应该选取什么样的值，来大道最好效果呢？&lt;br /&gt;
我们采用 k-fold cross validation 的方法。&lt;/p&gt;

&lt;p&gt;我们将训练集train分成k份，比如 k=5 的时候，
我们先用1，2，3，4来训练，5用来验证；&lt;br /&gt;
再用1，2，3，5来训练，4用来验证；
再用1，2，4，5来训练，3用来验证。。。
所以模型中创建了很多决策树。
我们测试每个分割方法下，参数每一个可能的取值，计算这个取值对应的预测精度，绘制曲线。&lt;br /&gt;
曲线的X轴是参数的取值，Y轴是预测精度，这样可以很容易找到参数的最佳取值。&lt;/p&gt;

&lt;p&gt;像R平方一样，我们也定义了一个概念 CP(complexity parameter) 用来观测效果。&lt;br /&gt;
cp越小，决策树越大(over fitting)。&lt;/p&gt;

&lt;h3 id=&#34;2-建模和评估:0257109cd77173fde404dfe977de0c33&#34;&gt;2.建模和评估&lt;/h3&gt;

&lt;h4 id=&#34;cart:0257109cd77173fde404dfe977de0c33&#34;&gt;CART&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Install rpart library
install.packages(&amp;quot;rpart&amp;quot;)
library(rpart)
install.packages(&amp;quot;rpart.plot&amp;quot;)
library(rpart.plot)

# CART model
# method=&amp;quot;class&amp;quot; 表示我们创建了一个 classification tree
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=&amp;quot;class&amp;quot;, minbucket=25)

# plot tree
prp(StevensTree)

# Make predictions
# 记得指定 type = &amp;quot;class&amp;quot;
PredictCART = predict(StevensTree, newdata = Test, type = &amp;quot;class&amp;quot;)
table(Test$Reverse, PredictCART)

# ROC curve
library(ROCR)

PredictROC = predict(StevensTree, newdata = Test)
# 注意这里没有指定 type = &amp;quot;class&amp;quot;
# 也就是说，学习得到 classification tree 的模型，但是评估使用 regression tree
# 真是天杀的。。。
# 这个PredictROC 有两列
# 第一列是预测y=0的概率
# 第二列是预测y=1的概率
# 如果比较一下 PredictROC 每行的数据，可以发现这两个概率和为1！那是当然！
# 如果拿 PredictROC 和 PredictCART相比
# 如果 PredictROC[n,2]&amp;gt;0.5，那么PredictCART[n]=1。
# 如果 PredictROC[n,2]&amp;lt;0.5，那么PredictCART[n]=0。
# 所以下面我们只使用第二列

pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(perf)

# AUC
as.numeric(performance(pred, &amp;quot;auc&amp;quot;)@y.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;random-forest-1:0257109cd77173fde404dfe977de0c33&#34;&gt;Random Forest&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;randomForest&amp;quot;)
library(randomForest)

# Build random forest model
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
# Warning message:
# In randomForest.default(m, y, ...) :
#   The response has five or fewer unique values.  Are you sure you want to do regression?

# 如上面的提示消息所示
# randomForest认为因变量的取值很少，不应该用regression
# 但是 random forest 没有 type = &amp;quot;class&amp;quot; 这样的参数
# 所以我们必须确保因变量这一列的取值都是因子
# Convert outcome to factor
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)

# Try again
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )

# Make predictions
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;cross-validation-1:0257109cd77173fde404dfe977de0c33&#34;&gt;Cross Validation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Install cross-validation packages
install.packages(&amp;quot;caret&amp;quot;)
library(caret)
install.packages(&amp;quot;e1071&amp;quot;)
library(e1071)

# Define cross-validation experiment
numFolds = trainControl( method = &amp;quot;cv&amp;quot;, number = 10 )
cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01)) 

# Perform the cross validation
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = &amp;quot;rpart&amp;quot;, trControl = numFolds, tuneGrid = cpGrid )

# Create a new CART model
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=&amp;quot;class&amp;quot;, cp = 0.18)

# Make predictions
PredictCV = predict(StevensTreeCV, newdata = Test, type = &amp;quot;class&amp;quot;)
table(Test$Reverse, PredictCV)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;参数cp和loss的使用:0257109cd77173fde404dfe977de0c33&#34;&gt;参数cp和loss的使用&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Penalty Matrix
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)

# CART model
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method=&amp;quot;class&amp;quot;, cp=0.00005)

prp(ClaimsTree)

# Make predictions
PredictTest = predict(ClaimsTree, newdata = ClaimsTest, type = &amp;quot;class&amp;quot;)
# New CART model with loss matrix
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method=&amp;quot;class&amp;quot;, cp=0.00005, parms=list(loss=PenaltyMatrix))

# Redo predictions and penalty error
PredictTest = predict(ClaimsTree, newdata = ClaimsTest, type = &amp;quot;class&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记03</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-26-R03/</link>
      <pubDate>Tue, 26 Apr 2016 11:52:13 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-26-R03/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第三单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;logistic-regression:2842374fb15231f36230fc5afd744bb4&#34;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;第三单元的主题是指数回归。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:2842374fb15231f36230fc5afd744bb4&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;指数回归:2842374fb15231f36230fc5afd744bb4&#34;&gt;指数回归&lt;/h4&gt;

&lt;p&gt;指数回归用于因变量y是二进制的情况，也就是说，y的取值只有1或者0。&lt;br /&gt;
y=1的概率：&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?P(y=1)=\frac{1}{1+e^{-{(\beta_0 +\beta_1x_1+\beta_2x_2+\ldots+\beta_nx_n+\epsilon)}}}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;y=1的概率与y＝0的概率的比值：&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=\frac{P(y=1)}{P(y=0)}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=\frac{P(y=1)}{1-P(y=1)}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=e^{\beta_0 +\beta_1x_1+\beta_2x_2+\ldots+\beta_nx_n+\epsilon}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;混淆矩阵-confusion-matrix:2842374fb15231f36230fc5afd744bb4&#34;&gt;混淆矩阵（confusion matrix）&lt;/h4&gt;

&lt;p&gt;有阈值t，&lt;br /&gt;
如果P(y=1) &amp;gt;=t，则预测y=1。&lt;br /&gt;
如果P(y=1) &amp;lt; t，则预测y=0。&lt;/p&gt;

&lt;p&gt;对于预测结果，我们得到矩阵&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;predict y=0&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;predict y=1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;actual y=0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TN (True  Nagative)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FP (False Positive)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;actual y=1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FN (False Nagative)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TP (True  Positive)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;根据矩阵中的值，我们可以计算指数回归的一些指标：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?accuracy=\frac{TN+TP}{N}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?specificity=\frac{TN}{TN+FP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?sensitivity=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;补充概念：&lt;br /&gt;
适合率&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?precision=\frac{TP}{FP+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
再现率&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?recall=tpr=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
F值（F-measure）&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?F-measure=\frac{2*precision*recall}{precision+recall}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
F值越高，性能越好&lt;/p&gt;

&lt;h4 id=&#34;roc曲线:2842374fb15231f36230fc5afd744bb4&#34;&gt;ROC曲线&lt;/h4&gt;

&lt;p&gt;ROC曲线 (Receiver Operator Characteristic curve)可以指导我们如何选取阈值t。
y轴的指标是 sensitivity，所以也叫 True positive rate。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?sensitivity=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
x轴的指标是 1-specificity，所以也叫 False positive rate。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?1-sensitivity=\frac{FP}{TN+FP}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每取一个阈值t，则计算相对应的 TPR 和 FPR，在坐标里标出这个点，就形成ROC曲线。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160509-ROC.png&#34; alt=&#34;ROC Curve&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t=0时，我们预测所有的y=1，即TPR=1，FPR=1，对应的坐标是(1,1)   
t=1时，我们预测所有的y=0，即TPR=0，FPR=0，对应的坐标是(0,0)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是曲线的两个端点。&lt;/p&gt;

&lt;h4 id=&#34;auc值:2842374fb15231f36230fc5afd744bb4&#34;&gt;AUC值&lt;/h4&gt;

&lt;p&gt;AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。&lt;/p&gt;

&lt;h3 id=&#34;2-建立回归模型:2842374fb15231f36230fc5afd744bb4&#34;&gt;2.建立回归模型&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 建立模型
# Top10作为因变量，其他所有的列都作为自变量
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)

# Top10作为因变量，除了loudness以外的所有列都作为自变量
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-评估:2842374fb15231f36230fc5afd744bb4&#34;&gt;3.评估&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 预测
testPredict = predict(SongsLog3, newdata=SongsTest, type=&amp;quot;response&amp;quot;)

# 生成混淆矩阵
table(SongsTest$Top10, testPredict &amp;gt;= 0.45)

# 生成ROC曲线
library(ROCR)
pred = prediction(testPredict, test$violator)
perf = performance(pred, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(perf)

# 加点颜色和坐标点
plot(perf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

# 计算AUC值
as.numeric(performance(pred, &amp;quot;auc&amp;quot;)@y.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;附录a-分割train和test的方法一:2842374fb15231f36230fc5afd744bb4&#34;&gt;附录A 分割train和test的方法一&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(caTools)
set.seed(144)

split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
# 特别注意：每次运行出来的结果是不一样的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(caTools)
set.seed(144)

split = sample(1:nrow(data), size=0.7 * nrow(data))
train = data[split,]
test = data[-split,]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;附录b-补充缺失数据:2842374fb15231f36230fc5afd744bb4&#34;&gt;附录B 补充缺失数据&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), &amp;quot;not.fully.paid&amp;quot;)
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记02</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-23-R02/</link>
      <pubDate>Sat, 23 Apr 2016 15:19:39 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-23-R02/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第二单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;linear-regression:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;Linear Regression&lt;/h2&gt;

&lt;p&gt;第二单元的主题是线性回归。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;1.理论&lt;/h3&gt;

&lt;p&gt;一元线性回归公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?y^i=\beta_0+\beta_1x^i+\epsilon^i&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中x是自变量independent variable，y是因变量dependent variable。&lt;br /&gt;
beta是相关系数coefficient，epsilon是误差error。&lt;/p&gt;

&lt;p&gt;为了判断线性回归的效果，我们有如下检验标准：&lt;/p&gt;

&lt;p&gt;1.SSE（sum of squared errors）&lt;br /&gt;
注意这里的误差是实际值相对于预测值的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?SSE = \sum_{i=1}^{n}\epsilon_i^2&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.SST （total sum of square）&lt;br /&gt;
公式同上。但这里的误差是实际值相对于baseline的。baseline是因变量的平均值。&lt;br /&gt;
所以有 0 &amp;lt;= SSE &amp;lt;= SST 。&lt;/p&gt;

&lt;p&gt;3.RMSE（root mean square error）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?RMSE = \sqrt\frac{SSE}{n}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.R平方&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?R^2 = 1 - \frac{SSE}{SST}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R平方越接近1越好。&lt;/p&gt;

&lt;p&gt;多元线性回归公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?y^i = \beta_0 + \beta_1x_1^i + \beta_2x_2^i + \ldots + \beta_nx_n^i + \epsilon^i&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;所有数据分析，都要经历 training－test－predict 这三个过程。
在接下来的例子中，我们介绍 建模－评估 这前两个过程。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;补充一个relative error的公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?relative\;error =  \frac{observed\;value - estimated\,value}{observed\;value}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;2-0-事前整理:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;2.0 事前整理&lt;/h5&gt;

&lt;p&gt;2.0.1 去除空值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 如果数据中包含空值
DF ＝ na.omit(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.0.2 reference level&lt;br /&gt;
有些列时字符型的，它们无法进行计算。&lt;br /&gt;
如果某列的因子不算多，我们可以把这一列变换成多个可以用于计算的列。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 假设DF$colr有因子 &amp;quot;Red&amp;quot;4次, &amp;quot;Blue&amp;quot;3次, &amp;quot;Yellow&amp;quot;2次
DF$colr = relevel(DF$colr, &amp;quot;red&amp;quot;)

# 效果是，DF$colr 这一列不见了
# 增加了两列 DF$colrBlue 和 DF$colrYellow
# 原先 DF$colr == &amp;quot;Red&amp;quot; 的那些行，它们 colrBlue 和 colrYellow 的值都是0
# 原先 DF$colr == &amp;quot;Blue&amp;quot; 的那些行，它们 colrBlue=1, colrYellow=0
# 原先 DF$colr == &amp;quot;Yellow&amp;quot; 的那些行，它们 colrBlue=0, colrYellow=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-建立回归模型:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;2.建立回归模型&lt;/h3&gt;

&lt;p&gt;建模使用lm()函数。&lt;br /&gt;
DF是保存学习数据的data.frame。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = lm(y ~ x1 + x2 + ... +xn, data = DF)
# y不要写成 DF$y
# x1也不要写成 DF$x1
# 否则，后面做预测predict()的时候，DFTest代入会报warning

# 除了y列以外所有列
model = lm(y ~ ., data = DF)

# 误差 model$residuals
SSE = sum(model$residuals^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随便看个结果吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; summary(model)

Call:
lm(formula = Price ~ HarvestRain + WinterRain, data = wine)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.0933 -0.3222 -0.1012  0.3871  1.1877 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  7.865e+00  6.616e-01  11.888 4.76e-11 ***
HarvestRain -4.971e-03  1.601e-03  -3.105  0.00516 ** 
WinterRain  -9.848e-05  9.007e-04  -0.109  0.91392    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5611 on 22 degrees of freedom
Multiple R-squared:  0.3177,    Adjusted R-squared:  0.2557 
F-statistic: 5.122 on 2 and 22 DF,  p-value: 0.01492
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Call表示建模使用的语句。&lt;br /&gt;
Residuals表示误差。&lt;br /&gt;
Coefficients表示系数，就是公式里面的beta。&lt;br /&gt;
Estimate的第一行是常数beta0，第二行是第一个自变量的系数beta1，第三行是第二个自变量的系数beta2，后面类推。&lt;br /&gt;
t value越大越好&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?{t\,value} = \frac{Estimate}{Std. Error}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Pr(&amp;gt;|t|) 和t value相反，越小越好。&lt;br /&gt;
最后一列星星越多越好。&lt;br /&gt;
三短横下面这行解释了星星的含义。&lt;br /&gt;
Multiple R-squared就是R平方，越接近1越准确。&lt;/p&gt;

&lt;h3 id=&#34;3-评估:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;3.评估&lt;/h3&gt;

&lt;p&gt;对于刚过简历的模型，我们使用测试数据来评估一下准确度。&lt;br /&gt;
model就是上文建立的模型。&lt;br /&gt;
DFTest是测试数据，它的结构和上文的DF一样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;predict = predict(model, newdata = DFTest)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个命令的返回值是 DFTest$Price 的&lt;strong&gt;预测&lt;/strong&gt;结果。你可以跟 DFTest$Price 的&lt;strong&gt;实际&lt;/strong&gt;结果相比较，计算SSE、RMSE、R平方等等来衡量对测试数据的预测的准确性。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SSE = sum( (DFTest$Price - predict)^2 )
SST = sum( (DFTest$Price - mean(DF$Price)^2 )
R2 = 1 - SSE/SST
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-correlation:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;4.Correlation&lt;/h3&gt;

&lt;p&gt;线性相关性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cor(var1, var2)
# 也可以考察整个DF中，每两列的线性相关性
cor(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回值是斜率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建立线性回归模型的时候，应该去掉相关性比较高的列。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;补充知识a-棒球统计术语:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识A－棒球统计术语&lt;/h3&gt;

&lt;p&gt;完全不懂棒球啊，一开始摸不着头脑。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;缩写&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;原文&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;中文&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Run Scores&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;跑分，得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Run Allowed&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失分，对手得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OBP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;On-Base Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;上垒率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OOBP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent On-Base Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;对手上垒率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SLG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Slugging Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;长打率，击中率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OSLG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent Slugging Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;对手长打率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Batting Avarage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;平均成功率&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;补充知识b-篮球统计术语:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识B－篮球统计术语&lt;/h3&gt;

&lt;p&gt;年轻时看NBA，好歹知道一点。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;缩写&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;原文&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;中文&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;PTS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;oppPTS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失分，对手得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Field Goals (success)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FGA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Field Goals Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X2P&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2 Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2分球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X2PA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2 Points Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2分球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X3P&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3 Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3分球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X3PA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3 Points Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3分球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Free Throw&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;罚球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FTA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Free Throw Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;发球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ORB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Offensive Rebounds&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;前场篮板，进攻篮板&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;DRB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Defensive Rebounds&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;后场篮板，防守篮板&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;AST&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Assists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;助攻&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;STL&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steals&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;抢断&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BLK&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Blocks&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;盖帽&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;TOV&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Turnovers&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失误&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;注：X2P列，原始数据列名是2P。由于R不支持数字开头的列名／变量，读取CSV文件的时候，会在原列名2P前加个X，从而变成 X2P。&lt;/p&gt;

&lt;h3 id=&#34;补充知识c-滞后序列:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识C－滞后序列&lt;/h3&gt;

&lt;p&gt;函数lag，用于生成滞后/偏移序列？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lag(x, k = 1, ...)
# k &amp;lt; 0, previous observations   
# k &amp;gt; 0, future observations
# na.pad=TRUE, add missing values
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记01</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-18-R01/</link>
      <pubDate>Mon, 18 Apr 2016 19:55:25 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-18-R01/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第一单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;r语言入门:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;R语言入门&lt;/h2&gt;

&lt;p&gt;R语言入门只讲了一些常用的操作。相对于动辄花一本书来讲这些，真是相当简约。但其实足够了，其他操作，需要的时候再查嘛。&lt;br /&gt;
数据分析的四要素：data、models、decisions、value&lt;/p&gt;

&lt;h3 id=&#34;简单使用:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;简单使用&lt;/h3&gt;

&lt;p&gt;帮助：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;?func
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示当前的临时变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取／设置当前目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getwd()
setwd()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示当前文件夹下的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据结构:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;向量概念：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;所有的操作都是对向量的每个元素实施的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;data.frame：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;observation：行
variable   ：列，data.frame是按照列存储的
rbind()合并两个data.frame
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;序列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;自动生成序列seq()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;获取数据:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;获取数据&lt;/h3&gt;

&lt;p&gt;读取CSV文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF = read.csv(&amp;quot;file_path&amp;quot;)
# 返回值是data.frame
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看DF的基本信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;str(DF)
# DF的结构信息。行和列的数目，列名、列的类型、列的数据举例。

summary(DF)
# 每列的最大值、最小值、中位数、平均数、1/4值、3/4值，以及是否包含空值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写入CSV文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;write.csv(DF, &amp;quot;file_path&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从内存中删除变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据操作:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;数据操作&lt;/h3&gt;

&lt;p&gt;选取一部分数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subset( DF, 条件1 &amp;amp; 条件2 ｜ 条件3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照列名选取3列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF[c(var1, var2, var3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选取1，3，5列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF[c(1, 3, 5)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算平均值和标准差：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mean(DF$var)
sd(DF$var)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回最大值／最小值的位置(index)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;which.max(DF$var)
which.min(DF$var)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回行数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nrow(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;绘图:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;绘图&lt;/h3&gt;

&lt;p&gt;直方图，反映&lt;strong&gt;一列&lt;/strong&gt;数据的分布情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hist(DF$var, xlim = c(1, 100), breaks = 100)
# xlim 限定范围
# breaks x轴的精确度。注意是针对原始数据的，不是对限定后的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;箱型图：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;boxplot(DF$var1 ~ DF$var2, xlab = &amp;quot;x-label&amp;quot;, ylab = &amp;quot;y-label&amp;quot;, main = &amp;quot;title&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点阵图：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plot(......, col = &amp;quot;red&amp;quot;)
line(......, col = &amp;quot;blue&amp;quot;) # 在原先的基础上再加一条

函数jitter() # 对于有很多重合的点阵图，先用jitter偏移一点，这样看上去效果好很多
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他共通的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col = &amp;quot;red&amp;quot;
type = &amp;quot;line&amp;quot; # 可以指定1，2，3，4，5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;聚合:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;聚合&lt;/h3&gt;

&lt;p&gt;分组：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table(DF$var1)
# var1列中，每种数据的数量的统计
table(DF$var1, DF$var2)
# var1和var2列中，每种数据的数量的交叉统计
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分组计算：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tapply(DF$var1, DF$var2, func)
# DF$var1, 原始数据
# DF$var2, 分组依据
# func, 要应用的函数
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>