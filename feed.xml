<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>行行重行行</title>
    <link>http://youngspring1.github.io/</link>
    <description>Recent content on 行行重行行</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2016. All rights reserved.</copyright>
    <lastBuildDate>Fri, 17 Jun 2016 20:53:25 +0800</lastBuildDate>
    <atom:link href="http://youngspring1.github.io/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>2016六月 孔庙泰山行记</title>
      <link>http://youngspring1.github.io/post/2016/2016-06-17-taishan/</link>
      <pubDate>Fri, 17 Jun 2016 20:53:25 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-06-17-taishan/</guid>
      <description>

&lt;h4 id=&#34;孔庙孔府孔林:93cd1a03fb24cb10fd9ad355849ea278&#34;&gt;孔庙孔府孔林&lt;/h4&gt;

&lt;p&gt;可能是宗教建筑意外，中国唯一历经各朝战火却能幸存的建筑了。建筑从宋到民国，并且在末尾一个小型的博物馆中看到了乙瑛碑，意外收获。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160611_sankong.jpg&#34; alt=&#34;三孔邮戳&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;泰山:93cd1a03fb24cb10fd9ad355849ea278&#34;&gt;泰山&lt;/h4&gt;

&lt;p&gt;传统的红门线路，2个多小时到南天门。虽然不是特别辛苦，但是人挤人真的很无聊，而且并没有多少风景可以看。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160617-taishan.JPG&#34; alt=&#34;五岳独尊&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;日常生活:93cd1a03fb24cb10fd9ad355849ea278&#34;&gt;日常生活&lt;/h4&gt;

&lt;p&gt;沉醉于旅途中这样的瞬间，窥见别人的日常生活。许久没有品尝到的人间烟火。&lt;br /&gt;
另外一个捷径是逛当地的菜市场，婺源清华镇、绩溪、大同、苏州……明显能感受到勃勃的生机。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160610_band.jpg&#34; alt=&#34;日常生活&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>训练专注力</title>
      <link>http://youngspring1.github.io/post/2016/2016-06-15-focus/</link>
      <pubDate>Wed, 15 Jun 2016 20:28:54 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-06-15-focus/</guid>
      <description>

&lt;h4 id=&#34;1-专注的状态:067ea3473217959ce25f78341d81828e&#34;&gt;1.专注的状态&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;流体验，指的是当人完全专注地投入一项事业中时，可以从中体验到的极度的快乐。沉浸在这种快乐当中的感受与一般的高兴不同，它是一种难得的高峰体验。人们在这时注意力会完全集中在当下的事物上，感觉不到自我的存在，感觉时间在不知不觉中飞逝，创造力和灵感得到激发，内心非常专注而平静，没有任何冲突，并且从自己所从事的事物中获得极大的满足。所谓的“废寝忘食”，就是沉浸在心流体验中的人们会出现的忘我状态。经常拥有这种高峰体验，会让人的内心得到升华，工作和生活进入到更高的境界，也是人充分实现自身潜能的一个重要途径。

这种高峰体验的出现又一些必要条件：首先就是需要人能毫无杂念地投入，全神贯注；另外，需要所做的事情和个人的能力很好地匹配，若是太过简单，人容易感觉无聊，太过复杂，人则容易感觉受挫；第三，人需要能在这个过程中不断地取得进展，比较明确的目标和及时的反馈都很重要，可以帮助人不断地调整自己的行动。有趣的是，人通常都是在非常积极地为事物努力的时候才会体验到这种极度的快乐。如果只是从事一些被动的活动，比如看电视等，哪怕它们会让人愉快，也不会激发心流。除此之外，精力涣散和心不在焉也会减少人们从事某项事物中获得的乐趣。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-进入状态-保持状态:067ea3473217959ce25f78341d81828e&#34;&gt;2.进入状态／保持状态&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;程序员们都知道，任务切换需要耗费许多额外的花销，通俗地来讲，首先需要保存当前上下文以便下次能够顺利切换回来，然后要加载目标任务的上下文。如果一个系统不停地在多个任务之间来回倒腾，就会耗费大量的时间在上下文切换上，无形中浪费很多的时间。

相比之下，如果只做一件任务，就不会有此损失。这就是为什么专注的人比不专注的人时间利用效率高得多的原因。任务切换的暗时间看似非常不明显，甚至很多人认为“多任务”是件很好的事情（有时候的确是），但日积月累起来就会发现，消耗在切换上的时间越来越多。

另外，大脑开始一件任务的时候必须要有一定时间来“热身”，这个时间因人而异，并且可以通过练习来改变。举个例子，你看了一会书之后，忽然感到一阵无聊，忍不住打开浏览器，十分钟后你想起来还要继续看书，但要回复到当时理想的状态，却需要一段时间来努力去集中精力，把记忆中相关的知识全都激活起来，从而才能进入“状态”，因为你上了十分钟网之后这些记忆已经被抑制了。如果这个“热身”状态需要一刻钟，那么看似十分钟的上网闲逛其实就花费了二十五分钟。

如果阅读的例子还不够生动，对于程序员来说其实有更好的例子：你写程序写得正high，忽然被叫去开了一通会，写到一半的代码搁在那儿。等你开完会回来你需要多久能够重新进入状态？又或者，你正在调试程序，你已经花了二十分钟的时间把与这个bug可能相关的代码前前后后都理解了一遍，心中构建了一个大致的地图，就在这时，呃，你又被叫去开了个会(:D)，开完会回来，可想而知，得花上一些时间来回想一下刚刚弄清的东西了。

迅速进入状态的能力是可以锻炼的，根据我个人的经验，至少可以缩短到3-5分钟。但要想完全进入状态，却是很难在这么短的时间实现的。所谓完全进入状态，举个例子：你看了3个小时的书，或者调试了半个小时的程序之后，往往满脑子都是相关的东西，所有这些知识都处在活跃状态，换言之你大脑中所有相关的记忆神经网络都被激活了，要达到这样一种忘记时间流逝的“沉浸”状态（心理学上叫做“流体验”），不是三两分钟的事情。而一旦这种状态被破坏，无形间效率就会大打折扣。这也是为什么我总是倾向于创造大块的时间来阅读重要的东西，因为这样有利于“沉浸”进去，使得新知识可以和大脑中与其相关的各种既有的知识充分融合，关联起来，后者对于深刻的记忆非常有帮助。

要充分利用暗时间，不仅要能够迅速进入状态，另一个很重要的习惯就是能够保持状态多久（思维体力）。

能够迅速进入专注状态，以及能够长期保持专注状态，是高效学习的两个最重要习惯。
值得庆幸的是，专注力和耐力与才能不同，可以通过训练于后天获得可以不断提升其资质。只要每天坐在书桌前，训练将意识倾注于一点，自然就能掌握。这同前面写过的强化肌肉的做法十分相似。每天不间断地写作集中意识去工作，这些非做不可----将这样的信息持续不断地传递给身体系统，让它牢牢地记住，再悄悄移动刻度，一点一点将极限值将上提升，注意不让身体发觉。这跟每天坚持慢跑，强化肌肉，逐步打造出跑步者的体型，乃是异曲同工。给它刺激，持续。再给它刺激，持续。这一过程当然需要耐心，不过一定会得到相应的回报。

优秀的侦探小说家雷蒙德.钱德勒在私信中说过：“哪怕没有什么东西可写，我私吞也肯定在书桌前坐上好几个小时，独自一人集中精力。”他这么做是为了什么，我完全能理解。钱德勒通过这么做，来提高职业作家必需的膂力，静静地提高士气。这样一种日常训练对他必不可缺。

我认为写作长篇小说是一种体力劳动。写文章属于脑力劳动，然而写出一本大部头来，更近于体力劳动。诚然，写书并不需要举起沉重的物体，也不需要飞速地奔来跑去，高高地蹿上跳下。世间的很多人似乎只看到表面，将作家的工作视为宁静而更改的书斋劳动，以为有了足以端起一只咖啡杯的力量，就能写小说了。试它一试，立即就会明白，写小说并非那么安逸的工作。坐在书桌前，将神经如同激光束一般集于一点，动用想象力，从“无”的地平线上催生出故事来，挑选出一个个正确的词语，让所有的流程准确无误----这样一种工作，与一般人想象的相比，更为长久地需要远为巨大的能量。这固然不必运动身体，劳筋动骨的劳动却在体内热火朝天地展开。固然，思索问题的是脑子，小说家却需披挂着叫“故事”的全副装备，动用全身进行思考，这要求作家彻底地驱使----在许多时候是奴役----肢体能力。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-排除干扰:067ea3473217959ce25f78341d81828e&#34;&gt;3.排除干扰&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;干吗还会欣羡我能坐稳在咖啡馆里呢？到咖啡馆原本就为着隔离而来，隔离自己的家，隔离善良的声音，隔离掉所有熟悉、舒适、温暖的东西；正在写长篇的小说家林俊颖一人独居，如今却也冲出到咖啡馆来，他笑着说，书架一直在那里叫你，你一碰到困难，借口翻翻资料，寻找感觉，接下来你就发现自己又埋进某本书、某部小说里两小时了。所以，所有像回事的作家最终几乎都在早上书写，趁着整个世界才刚醒来，还跟你暂时处在一种相互隔离的状态，你还有能力把它当在外头——就连海明威这种浮夸好热闹的人都告诉我们，在早晨进入写作之前，不做其他任何有企图心的事；纳博科夫一致工作到下午，知道黄昏散步时才找报纸看，才放世界溜进来；在淡水写作的舞鹤甚至不读报，他只在喂食镇上街猫时顺便瞄一眼头条，知道没发生战争，末日还没来就可以了。
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-养成习惯:067ea3473217959ce25f78341d81828e&#34;&gt;4.养成习惯&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;书中主人翁流落赌城，在绝望时刻偶然从一个老头手上得到一个必然赢钱的赌方，但这个最后一定大赢的赌方非常诡异非常磨人，它必须先挨过一定阶段的输钱，只能输不能赢，而且明知是输亦一步也不能省——写小说的格林迷朱天心尤其喜欢这个例子，她在新小说顺利开笔之前，一样总要经历这同样的短则数日长可数星期的枯坐思索（在小说题材乃至内容已经完全锁定备妥的情况下），明明知道一定空手而回仍得每天带着书、草稿本和笔到写作的咖啡馆报到，她出门时的口头禅便是：“去输钱”。
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>善哉行</title>
      <link>http://youngspring1.github.io/post/2016/2016-06-06-lonely/</link>
      <pubDate>Mon, 06 Jun 2016 22:46:00 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-06-06-lonely/</guid>
      <description>

&lt;h2 id=&#34;善哉行:fc69331b0f799d0bad7de7a3734ff807&#34;&gt;善哉行&lt;/h2&gt;

&lt;h4 id=&#34;曹丕:fc69331b0f799d0bad7de7a3734ff807&#34;&gt;曹丕&lt;/h4&gt;

&lt;p&gt;上山采薇，薄暮苦饥。&lt;/p&gt;

&lt;p&gt;溪谷多风，霜露沾衣。&lt;/p&gt;

&lt;p&gt;野雉群雊，猿猴相追。&lt;/p&gt;

&lt;p&gt;还望故乡，郁何垒垒！&lt;/p&gt;

&lt;p&gt;高山有崖，林木有枝。&lt;/p&gt;

&lt;p&gt;忧来无方，人莫之知。&lt;/p&gt;

&lt;p&gt;人生如寄，多忧何为？&lt;/p&gt;

&lt;p&gt;今我不乐，岁月如驰。&lt;/p&gt;

&lt;p&gt;汤汤川流，中有行舟。&lt;/p&gt;

&lt;p&gt;随波转薄，有似客游。&lt;/p&gt;

&lt;p&gt;策我良马，被我轻裘。&lt;/p&gt;

&lt;p&gt;载驰载驱，聊以忘忧。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记07</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-24-R07/</link>
      <pubDate>Tue, 24 May 2016 09:18:29 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-24-R07/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第七单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;visualization:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;Visualization&lt;/h2&gt;

&lt;p&gt;第七单元的主题是可视化。&lt;/p&gt;

&lt;h3 id=&#34;1-简介:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;1.简介&lt;/h3&gt;

&lt;p&gt;plot和ggplot2的比较&lt;br /&gt;
plot：只有简单的点和线，不容易添加其他元素。&lt;br /&gt;
ggplot2：引入图层，很容易添加其他元素&lt;/p&gt;

&lt;h4 id=&#34;ggplot2:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;ggplot2&lt;/h4&gt;

&lt;p&gt;ggplot2三要素：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data&lt;br /&gt;
数据，使用data.frame。&lt;/li&gt;
&lt;li&gt;Aesthetic mapping&lt;br /&gt;
指定如何将 data.frame里的变量映射到图形属性上。比如，颜色，形状，比例，x／y坐标，分组等等。&lt;/li&gt;
&lt;li&gt;Geometric objects&lt;br /&gt;
决定数据以什么样的形式显示。比如，点，线，箱线图，条形图，多边形等等。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;结合下面这条命令，参数WHO就是提供数据的data.frame，参数aes()就是Aesthetic mapping，后面用加号连结的类似geom_point()就是Geometric objects。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 形式
# ggplot(data = NULL, mapping = aes(), ..., environment = parent.frame())
# 例子
ggplot(WHO, aes(x = GNI, y = FertilityRate, color = Region)) + geom_point()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ase()即可以作为ggplot()的参数，又可以作为geom_XXXX()的参数&lt;/p&gt;

&lt;h4 id=&#34;aesthetic-mapping:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;Aesthetic mapping&lt;/h4&gt;

&lt;p&gt;坐标相关&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aes(x, y, xmin, xmax, ymin, ymax, xend, yend)
# 当然就是x，y坐标分别指定data.frame的某一列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注：坐标相关的，一般作为ggplot()的参数，其他的都可以作为geom()的参数。&lt;/p&gt;

&lt;h4 id=&#34;geometric-objects:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;Geometric objects&lt;/h4&gt;

&lt;p&gt;颜色相关&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aes(colour, fill, alpha)
# colour 颜色
# fill   填充指标，data.frame的某一列。也类似于分类，比如该列有两个因子，那么会用两种不同的颜色填充
# alpha  透明度，0到1之间的小数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分组相关&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aes(group)
# group 分组指标，可以指定为1，那所有数据都在1组。也可以指定data.frame的某一列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;形态相关&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aes(linetype, size, shape)
# linetype 即lty，线段的类型
# size     点的大小，线的粗细。指定整数数值。
# shape    图形的类型
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;图形的类型，即geom_point(shape = n)中n的取值&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160524-shapes.png&#34; alt=&#34;shapes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;线段的类型，即geom_point(lty = n)中n的取值&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160524-line-types.png&#34; alt=&#34;line-types&#34; /&gt;&lt;/p&gt;

&lt;p&gt;描绘形状&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;geom_point()  点
geom_line()   线
geom_tile()   条形图
geom_bar()    直方图
geom_ploygen()多边形
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注：&lt;br /&gt;
binwidth = 5 :粒度？&lt;br /&gt;
geom_bar(stat=&amp;ldquo;identity&amp;rdquo;) :use the value of the y variable as is&lt;br /&gt;
geom_histogram(position = &amp;ldquo;identity&amp;rdquo;) :not to stack the histograms&lt;/p&gt;

&lt;h3 id=&#34;2-实战:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;2.实战&lt;/h3&gt;

&lt;h4 id=&#34;绘图:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;绘图&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Read in data
WHO = read.csv(&amp;quot;WHO.csv&amp;quot;)
str(WHO)

# Plot from Week 1
plot(WHO$GNI, WHO$FertilityRate)

# Let&#39;s redo this using ggplot 
# Install and load the ggplot2 library:
install.packages(&amp;quot;ggplot2&amp;quot;)
library(ggplot2)

# Create the ggplot object with the data and the aesthetic mapping:
scatterplot = ggplot(WHO, aes(x = GNI, y = FertilityRate))

# Add the geom_point geometry
scatterplot + geom_point()

# Make a line graph instead:
scatterplot + geom_line()

# Switch back to our points:
scatterplot + geom_point()

# Redo the plot with blue triangles instead of circles:
scatterplot + geom_point(color = &amp;quot;blue&amp;quot;, size = 3, shape = 17) 

# Another option:
scatterplot + geom_point(color = &amp;quot;darkred&amp;quot;, size = 3, shape = 8) 

# Add a title to the plot:
scatterplot + geom_point(colour = &amp;quot;blue&amp;quot;, size = 3, shape = 17) + ggtitle(&amp;quot;Fertility Rate vs. Gross National Income&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;分组:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;分组&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# 因子，以颜色区分  
# Color the points by region: 
ggplot(WHO, aes(x = GNI, y = FertilityRate, color = Region)) + geom_point()

# 数值，以颜色深浅区分
# Color the points according to life expectancy:
ggplot(WHO, aes(x = GNI, y = FertilityRate, color = LifeExpectancy)) + geom_point()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;拟合:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;拟合&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Is the fertility rate of a country was a good predictor of the percentage of the population under 15?
ggplot(WHO, aes(x = FertilityRate, y = Under15)) + geom_point()

# Let&#39;s try a log transformation:
ggplot(WHO, aes(x = log(FertilityRate), y = Under15)) + geom_point()

# Simple linear regression model to predict the percentage of the population under 15, using the log of the fertility rate:
mod = lm(Under15 ~ log(FertilityRate), data = WHO)
summary(mod)

# Add this regression line to our plot:
ggplot(WHO, aes(x = log(FertilityRate), y = Under15)) + geom_point() +  stat_smooth(method = &amp;quot;lm&amp;quot;)

# 99% confidence interval
ggplot(WHO, aes(x = log(FertilityRate), y = Under15)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;, level = 0.99)

# No confidence interval in the plot
ggplot(WHO, aes(x = log(FertilityRate), y = Under15)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)

# Change the color of the regression line:
ggplot(WHO, aes(x = log(FertilityRate), y = Under15)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;, colour = &amp;quot;orange&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;热力图:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;热力图&lt;/h4&gt;

&lt;p&gt;热力图（数据越多颜色越深）的效果，依靠scale_fill_gradient()来实现，可以通过low和high指定深浅区域的颜色，然后自动形成渐变效果。旁边的图例通过参数guide = &amp;ldquo;legend&amp;rdquo;来指定。&lt;br /&gt;
最终的命令如下，如何生成数据的，就不啰嗦了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Change the color scheme
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(name=&amp;quot;Total MV Thefts&amp;quot;, low=&amp;quot;white&amp;quot;, high=&amp;quot;red&amp;quot;) + theme(axis.title.y = element_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;地理热力图:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;地理热力图&lt;/h4&gt;

&lt;p&gt;顾名思义，地理热力图就是在地图上显示热力图。&lt;br /&gt;
包map内置了美国地图、世界地图、法国地图、意大利地图等。地图的原理跟图片类似，图片就是按照某个粒度分成很多个像素点，然后保存像素点的颜色信息；地图就是按照经纬度分成很多点，保存每个点的信息（比如这个点位于哪个州，这样就形成一个美国地图）。
对比刚才的 ggplot() + geom_tile() + scale_fill_gradient()&lt;br /&gt;
我们现在使用 ggmap() + geom_point() + scale_fill_gradient()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Install and load two new packages:
install.packages(&amp;quot;maps&amp;quot;)
install.packages(&amp;quot;ggmap&amp;quot;)
library(maps)
library(ggmap)

# Load a map of Chicago into R:
chicago = get_map(location = &amp;quot;chicago&amp;quot;, zoom = 11)

# Look at the map
ggmap(chicago)

# Plot the first 100 motor vehicle thefts:
ggmap(chicago) + geom_point(data = mvt[1:100,], aes(x = Longitude, y = Latitude))

# Round our latitude and longitude to 2 digits of accuracy, and create a crime counts data frame for each area:
LatLonCounts = as.data.frame(table(round(mvt$Longitude,2), round(mvt$Latitude,2)))

str(LatLonCounts)

# Convert our Longitude and Latitude variable to numbers:
LatLonCounts$Long = as.numeric(as.character(LatLonCounts$Var1))
LatLonCounts$Lat = as.numeric(as.character(LatLonCounts$Var2))

# Plot these points on our map:
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq))

# Change the color scheme:
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq)) + scale_colour_gradient(low=&amp;quot;yellow&amp;quot;, high=&amp;quot;red&amp;quot;)

# We can also use the geom_tile geometry
ggmap(chicago) + geom_tile(data = LatLonCounts, aes(x = Long, y = Lat, alpha = Freq), fill=&amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;云图:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;云图&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# 先准备下数据，我们需要很多单词。
# 跟文本处理类似，依旧使用tweets推文，只是我们这次不抽取词干。
library(tm)
tweets = read.csv(&amp;quot;tweets.csv&amp;quot;, stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords(&amp;quot;english&amp;quot;))
frequencies = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(frequencies))

# 我们需要的单词就是列名
colnames(allTweets)
# 我们需要的另一个指标是单词的频率
colSums(allTweets)

# 现在加载wordcloud这个包
library(wordcloud)
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, .25))

# 参数 scale 指定了文字的大小
# scale=c(2, .25) 表示出现频率最高的单词，显示的字号为2，出现频率最小的单词，显示的字号为0.25
wordcloud(colnames(allTweets), colSums(allTweets))
# 等效于
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(4, 0.5))

# min.freq
# 只显示出现频率大于指定值的单词

# max.words
# 最多只显示指定数目的单词

# random.order == FALSE
# 最先显示出现频率最高的单词

# rot.per = 0.5
# 有一半的单词垂直显示。默认值是0.1。

# random.color == TRUE
# 使用随机颜色
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;颜色&lt;br /&gt;
包RColorBrewer支持下面这些调色板，可以输入 display.brewer.all() 看到下面这张图。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160606-brewer.all.png&#34; alt=&#34;brewer.all&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ibrary(RColorBrewer)
display.brewer.all()

# 像这样使用
colors=brewer.pal(9, &amp;quot;Blues&amp;quot;)[5:9]
wordcloud(colnames(allTweets), colSums(allTweets), colors)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;保存:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;保存&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Save our plot:
fertilityGNIplot = scatterplot + geom_point(colour = &amp;quot;blue&amp;quot;, size = 3, shape = 17) + ggtitle(&amp;quot;Fertility Rate vs. Gross National Income&amp;quot;)
pdf(&amp;quot;MyPlot.pdf&amp;quot;)
print(fertilityGNIplot)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;附录:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;附录&lt;/h3&gt;

&lt;h6 id=&#34;r中星期的显示:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;R中星期的显示&lt;/h6&gt;

&lt;p&gt;在中文系统上，weekdays()返回的结果是 “星期二 星期六 星期日 星期三 星期四 星期五 星期一”，如果希望输出的结果是“Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday”，应该怎么做？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Convert the Date variable to a format that R will recognize:
mvt$Date = strptime(mvt$Date, format=&amp;quot;%m/%d/%y %H:%M&amp;quot;)
mvt$Weekday = weekdays(mvt$Date)

table(mvt$Weekday)
星期二 星期六 星期日 星期三 星期四 星期五 星期一 
26791  27118  26316  27416  27319  29284  27397 

Sys.getlocale()
&amp;quot;zh_CN.UTF-8/zh_CN.UTF-8/zh_CN.UTF-8/C/zh_CN.UTF-8/zh_CN.UTF-8&amp;quot;
Sys.setlocale(&amp;quot;LC_TIME&amp;quot;, &amp;quot;en_US.UTF-8&amp;quot;)
&amp;quot;en_US&amp;quot;
Sys.getlocale()
&amp;quot;zh_CN.UTF-8/zh_CN.UTF-8/zh_CN.UTF-8/C/en_US.UTF-8/zh_CN.UTF-8&amp;quot;

table(mvt$Weekday)
Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday 
29284     27397     27118     26316     27319     26791     27416
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外注意到，不管是中文还是英文，都是按照字母表顺序排列的，不是按照实际中有意义的顺序排列的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WeekdayCounts = as.data.frame(table(mvt$Weekday))
WeekdayCounts$Var1 = factor(WeekdayCounts$Var1, ordered=TRUE, levels=c(&amp;quot;Sunday&amp;quot;, &amp;quot;Monday&amp;quot;, &amp;quot;Tuesday&amp;quot;, &amp;quot;Wednesday&amp;quot;, &amp;quot;Thursday&amp;quot;, &amp;quot;Friday&amp;quot;,&amp;quot;Saturday&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id=&#34;factor转数字:8f9383a8559e56d2b013b78edf4886b3&#34;&gt;factor转数字&lt;/h6&gt;

&lt;p&gt;先把factor转成character，再转成数字&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Convert the second variable, Var2, to numbers and call it Hour:
DayHourCounts$Hour = as.numeric(as.character(DayHourCounts$Var2))
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;参考：&lt;br /&gt;
&lt;a href=&#34;http://www.cookbook-r.com/Graphs/Shapes_and_line_types/&#34;&gt;形状和线段的类型&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)&#34;&gt;颜色&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记06</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-18-R06/</link>
      <pubDate>Wed, 18 May 2016 16:40:09 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-18-R06/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第六单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;clustering:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;Clustering&lt;/h2&gt;

&lt;p&gt;第六单元的主题是集群。它用来找到数据内的相似性。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;recommendation-systems:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;Recommendation Systems&lt;/h4&gt;

&lt;p&gt;Collaborative filtering:&lt;br /&gt;
过滤出用户间的共同特征／相似性。只使用了用户信息，跟电影内容本身无关。&lt;/p&gt;

&lt;p&gt;Content filtering:&lt;br /&gt;
利用电影本身的信息，过滤出有共同导演／演员／类别的电影。跟其他用户无关。&lt;/p&gt;

&lt;h4 id=&#34;clustering-1:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;clustering&lt;/h4&gt;

&lt;p&gt;clustering 集群是一种非监督学习，&amp;rdquo;unsupervised learning&amp;rdquo;，将有共同特征的数据分在同一组。&lt;/p&gt;

&lt;h6 id=&#34;hierarchical-clustering:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;Hierarchical clustering&lt;/h6&gt;

&lt;p&gt;Hierarchical clustering的步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计算距离&lt;/li&gt;
&lt;li&gt;生成集群&lt;/li&gt;
&lt;li&gt;生成cutree&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意1:计算距离时，有可能造成内存溢出。计算每两点间的距离，得到的结果是n*(n-1)/2个，我们需要保存这个结果，如果n很大，保存结果的矩阵也很大，可能会导致内存溢出。&lt;br /&gt;
注意2:计算距离的三种方法：&lt;br /&gt;
Euclidean distance：点与点之间的欧几里得距离&lt;br /&gt;
Manhattan Distance：绝对值之和&lt;br /&gt;
Maximum Coordinate：偏离最严重的点&lt;/p&gt;

&lt;h6 id=&#34;k-means-clustering:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;K-means clustering&lt;/h6&gt;

&lt;p&gt;K-means clustering的步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;指定集群数目k&lt;/li&gt;
&lt;li&gt;随机分配所有的点&lt;/li&gt;
&lt;li&gt;计算每个集群的中心点&lt;/li&gt;
&lt;li&gt;计算每个点到这些中心点的距离，选择最近的，重新分配点到离他最近的集群&lt;/li&gt;
&lt;li&gt;重新计算每个集群的中心点&lt;/li&gt;
&lt;li&gt;重复4和5多次，直到没有提升&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意：centroid distance 集群中所有点的平均值间的距离。&lt;/p&gt;

&lt;h4 id=&#34;normalize:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;normalize&lt;/h4&gt;

&lt;p&gt;如果不同列的数值不是同样的数量级，那么运算后较小的值可能会被忽略，所以需要调整到同样的数量级。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(caret)
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果就是，所有列的平均值都是0。&lt;/p&gt;

&lt;h3 id=&#34;2-建模和评估:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;2.建模和评估&lt;/h3&gt;

&lt;h4 id=&#34;hierarchical-clustering-1:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;Hierarchical clustering&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# After following the steps in the video, load the data into R
movies = read.table(&amp;quot;movieLens.txt&amp;quot;, header=FALSE, sep=&amp;quot;|&amp;quot;,quote=&amp;quot;\&amp;quot;&amp;quot;)
# Add column names
colnames(movies) = c(&amp;quot;ID&amp;quot;, &amp;quot;Title&amp;quot;, &amp;quot;ReleaseDate&amp;quot;, &amp;quot;VideoReleaseDate&amp;quot;, &amp;quot;IMDB&amp;quot;, &amp;quot;Unknown&amp;quot;, &amp;quot;Action&amp;quot;, &amp;quot;Adventure&amp;quot;, &amp;quot;Animation&amp;quot;, &amp;quot;Childrens&amp;quot;, &amp;quot;Comedy&amp;quot;, &amp;quot;Crime&amp;quot;, &amp;quot;Documentary&amp;quot;, &amp;quot;Drama&amp;quot;, &amp;quot;Fantasy&amp;quot;, &amp;quot;FilmNoir&amp;quot;, &amp;quot;Horror&amp;quot;, &amp;quot;Musical&amp;quot;, &amp;quot;Mystery&amp;quot;, &amp;quot;Romance&amp;quot;, &amp;quot;SciFi&amp;quot;, &amp;quot;Thriller&amp;quot;, &amp;quot;War&amp;quot;, &amp;quot;Western&amp;quot;)
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
# Remove duplicates
movies = unique(movies)

# Compute distances
distances = dist(movies[2:20], method = &amp;quot;euclidean&amp;quot;)

# Hierarchical clustering
# clusterMovies = hclust(distances, method = &amp;quot;ward&amp;quot;) 
clusterMovies = hclust(distances, method = &amp;quot;ward.D&amp;quot;)

# Plot the dendrogram
plot(clusterMovies)

# Assign points to clusters
clusterGroups = cutree(clusterMovies, k = 10)
# Create a new data set with just the movies from cluster 2
cluster2 = subset(movies, clusterGroups==2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;k-means-clustering-1:98d5e9e5464c079d1b4ca9841cba7675&#34;&gt;K-means clustering&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;healthy = read.csv(&amp;quot;healthy.csv&amp;quot;, header=FALSE)
# 注意
# data.frame-&amp;gt;matrix-&amp;gt;vector 变成一个2500的vector
# data.frame-&amp;gt;vector 还是一个50*50的data.frame
healthyMatrix = as.matrix(healthy)
healthyVector = as.vector(healthyMatrix)

# Specify number of clusters
k = 5
# Run k-means
set.seed(1)
KMC = kmeans(healthyVector, centers = k, iter.max = 1000)

# Extract clusters
healthyClusters = KMC$cluster

# Plot the image with the clusters
dim(healthyClusters) = c(nrow(healthyMatrix), ncol(healthyMatrix))

image(healthyClusters, axes = FALSE, col=rainbow(k))

# Apply to a test image
tumor = read.csv(&amp;quot;tumor.csv&amp;quot;, header=FALSE)
tumorMatrix = as.matrix(tumor)
tumorVector = as.vector(tumorMatrix)

# Apply clusters from before to new image, using the flexclust package
# kcca K-Centroids Cluster Analysis
install.packages(&amp;quot;flexclust&amp;quot;)
library(flexclust)
KMC.kcca = as.kcca(KMC, healthyVector)
tumorClusters = predict(KMC.kcca, newdata = tumorVector)

# Visualize the clusters
dim(tumorClusters) = c(nrow(tumorMatrix), ncol(tumorMatrix))
image(tumorClusters, axes = FALSE, col=rainbow(k))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记05</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-R05/</link>
      <pubDate>Sat, 14 May 2016 23:19:28 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-R05/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第五单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;text-analytics:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;Text Analytics&lt;/h2&gt;

&lt;p&gt;第五单元的主题是文本分析。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;bag-of-words:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;Bag of Words&lt;/h4&gt;

&lt;p&gt;一段文本，可以看作是多个单词的集合。&lt;br /&gt;
统计这些单词的特征，可以归纳文本的倾向。&lt;/p&gt;

&lt;p&gt;首先，我们需要对文本进行下面这几步预处理：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;clean up irregularities(统一大小写)&lt;/li&gt;
&lt;li&gt;remove punctuations(去掉标点或者特殊符号)&lt;/li&gt;
&lt;li&gt;remove stop words(去掉the／who／is／do这些单词)&lt;/li&gt;
&lt;li&gt;stemming(获取词干，也就是去除动词变形，比如agrued，agrues，agruing，都变成agru)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后，我们统计文本中剩下这些单词的出现次数，生成一个矩阵，类似这样的格式：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;text num&lt;/th&gt;
&lt;th&gt;word1&lt;/th&gt;
&lt;th&gt;word2&lt;/th&gt;
&lt;th&gt;word3&lt;/th&gt;
&lt;th&gt;&amp;hellip;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;text1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;text2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在实际中，生成的矩阵是个稀疏矩阵（有很多0），我们只选取出现次数比较多的，忽略那些不常见的单词。&lt;br /&gt;
比如选取至少出现过20次的单词，其他的忽略。&lt;br /&gt;
这样的矩阵，每列的列名就是自变量，矩阵的值就用做自变量的取值。&lt;/p&gt;

&lt;p&gt;最后，手动添加一列，作为因变量，这样就可以根据这些单词的出现次数，预测因变量的取值了。&lt;br /&gt;
所以，这一列因变量的数值如何定义，它的实际意义是什么，其实是比较复杂的。&lt;/p&gt;

&lt;p&gt;在课程的例子中，它定义了&amp;rdquo;好感度&amp;rdquo;，并且只有下面五种取值，{-2,-1,0,1,2}，最终要建立模型预测哪些文本暗示发推的人对苹果公司很没有好感（好感度是－2）。最终发现，文本中含有&amp;rdquo;hate&amp;rdquo;,&amp;ldquo;wtf&amp;rdquo;的情况，推主对苹果公司很没有好感。😄&lt;/p&gt;

&lt;h4 id=&#34;ibm-watson:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;IBM Watson&lt;/h4&gt;

&lt;p&gt;Watson的工作步骤是这样的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Find LAT&lt;br /&gt;
首先得搞明白问题是什么，也就是要找到问题的LAT(Lexial Answer Type)。
问题&amp;rdquo;Mozart&amp;rsquo;s last and perhaps most powerful symphony shares its name with this planet.&amp;ldquo;的LAT是&amp;rdquo;this planet&amp;rdquo;，因为把答案&amp;rdquo;Jupiter&amp;rdquo;替换进原来的句子，
&amp;ldquo;Mozart&amp;rsquo;s last and perhaps most powerful symphony shares its name with Jupiter&amp;rdquo;
仍然是说得通的。&lt;/li&gt;
&lt;li&gt;Generate Hypothesis&lt;br /&gt;
在数据库中搜索上百个候选答案，替换掉LAT，生成很多假说。&lt;/li&gt;
&lt;li&gt;Score Hypothesis&lt;br /&gt;
对每个假说，进行文本搜索，可以将搜索的到的结果数目作为评分。&lt;/li&gt;
&lt;li&gt;Rank Hypothesis&lt;br /&gt;
对评分进行排序，选取评分最高的那个作为答案。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-建模和评估:d5c69e163a15a2e5f9829d2828e73190&#34;&gt;2.建模和评估&lt;/h3&gt;

&lt;p&gt;预处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Read in the data
# 不要把文本转化为因子
tweets = read.csv(&amp;quot;tweets.csv&amp;quot;, stringsAsFactors=FALSE)

# Create dependent variable
tweets$Negative = as.factor(tweets$Avg &amp;lt;= -1)

# Install new packages
install.packages(&amp;quot;tm&amp;quot;)
library(tm)
install.packages(&amp;quot;SnowballC&amp;quot;)
library(SnowballC)

# Create corpus
corpus = Corpus(VectorSource(tweets$Tweet))

# Convert to lower-case
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)

# Remove punctuation
corpus = tm_map(corpus, removePunctuation)

# Remove stopwords and apple
corpus = tm_map(corpus, removeWords, c(&amp;quot;apple&amp;quot;, stopwords(&amp;quot;english&amp;quot;)))

# Stem document 
corpus = tm_map(corpus, stemDocument)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;统计，生成单词出现次数的矩阵&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Create matrix
frequencies = DocumentTermMatrix(corpus)

# Look at matrix 
inspect(frequencies[1000:1005,505:515])

# Check for sparsity
# 找出出现次数至少有20次的单词
findFreqTerms(frequencies, lowfreq=20)

# 忽略99.5%的稀疏数据，只选取0.5%作为有效数据
# Remove sparse terms 
sparse = removeSparseTerms(frequencies, 0.995)

# Convert to a data frame
tweetsSparse = as.data.frame(as.matrix(sparse))
# Make all variable names R-friendly
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))

# Add dependent variable
tweetsSparse$Negative = tweets$Negative
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建模和评估&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Split the data
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)

# Build a CART model
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data=trainSparse, method=&amp;quot;class&amp;quot;)
# Evaluate the performance of the model
predictCART = predict(tweetCART, newdata=testSparse, type=&amp;quot;class&amp;quot;)
table(testSparse$Negative, predictCART)


# Random forest model
library(randomForest)
set.seed(123)
tweetRF = randomForest(Negative ~ ., data=trainSparse)
# Make predictions:
predictRF = predict(tweetRF, newdata=testSparse)
table(testSparse$Negative, predictRF)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>眼前：漫游在《左传》的世界</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-zuozhuan/</link>
      <pubDate>Tue, 10 May 2016 23:20:31 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-zuozhuan/</guid>
      <description>&lt;p&gt;后人看历史的时候，不可避免地站在上帝视角思考问题。&lt;br /&gt;
读史记，一个人将来能够成就什么，从一开始就预设好了。&lt;br /&gt;
又或者大部分历史书，一个王朝如何兴起、成长、强盛、衰落，仿佛就是按照剧本来演的。&lt;/p&gt;

&lt;p&gt;而实际上，当时的人，哪知道后来会发生什么，哪知道做了某件事情的结果，甚至不知道大国的另外一个角落，此刻发生了什么。&lt;/p&gt;

&lt;p&gt;就好像看比赛直播，心情各种复杂，岂止是晚间新闻报道里三言两语能够概括的。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;亚马逊链接：&lt;a href=&#34;https://www.amazon.cn/gp/product/B01AU85S80/ref=od_aui_detailpages00?ie=UTF8&amp;amp;psc=1&#34;&gt;眼前：漫游在《左传》的世界&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2016五月 西安洛阳行记</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-xianluoyang/</link>
      <pubDate>Tue, 10 May 2016 23:16:19 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-xianluoyang/</guid>
      <description>

&lt;h4 id=&#34;西安印象:bffe2889357b458c59c153b34307fc87&#34;&gt;西安印象&lt;/h4&gt;

&lt;p&gt;很热闹的城市，超多游人。&lt;br /&gt;
吃的东西，还是自己找小巷子更好。&lt;br /&gt;
市区或者有名的文保单位明显不及山西的高质量。去了大雁塔50门票+30登塔，顿时发现佛光寺就算收150页不算贵。（啊啊啊，可怜我硬盘数据丢失，两三年的照片都没了）&lt;br /&gt;
最后发张图证明来过西安：&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160522-dyt.JPG&#34; alt=&#34;大雁塔&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;洛阳印象:bffe2889357b458c59c153b34307fc87&#34;&gt;洛阳印象&lt;/h4&gt;

&lt;p&gt;超棒的小城市，特别喜欢当地人生活的小巷子，特别有人气！（当然让我自己去住我是不愿意的）&lt;br /&gt;
可为什么大街上都是医院和卖成人用品的店呢？&lt;br /&gt;
白马寺和龙门石窟都超赞！&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160522-bms.JPG&#34; alt=&#34;白马寺&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160522-lmsk.JPG&#34; alt=&#34;龙门石窟&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;郑州印象:bffe2889357b458c59c153b34307fc87&#34;&gt;郑州印象&lt;/h4&gt;

&lt;p&gt;脏乱差。要不是这次河南博物院大修，我是不愿意来第二次的。&lt;/p&gt;

&lt;h4 id=&#34;多余的话:bffe2889357b458c59c153b34307fc87&#34;&gt;多余的话&lt;/h4&gt;

&lt;p&gt;那些被技术淘汰的事物：旅游地图／景区照相&lt;br /&gt;
以前到达一个地方，总会立马买一张当地的地图。是的，对我来说，这就是外部世界的入口。而如今大家早已经用手机地图了，在火车站／汽车站，还是能够遇到不少这样的老奶奶，举着地图，“5元一份”，看到这样的情景总是很纳闷，难道没有子女提醒他们，现在大部分人已经不买这个了吗？&lt;br /&gt;
同样还有景区快照。在以前数码相机还没有普及的时候，无可厚非。但是现在还大规模的打出“5分钟快照”的旗号，你们的世界就不更新吗？除非占据了特别好的机位（只有这个点才能拍到全景，而且我占了，旅游者自己拍不到），或者抓住了时机（比如坐过山车时）。&lt;/p&gt;

&lt;p&gt;停车场&lt;br /&gt;
秦始皇兵马俑附近农家跑到马路上招揽停车的游人，口号是“只要5块，景区10块”。你们其实没有想到，他们不在乎这五块十块钱，他们在乎的是能不能少走两步路，不谢。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记04</title>
      <link>http://youngspring1.github.io/post/2016/2016-05-10-R04/</link>
      <pubDate>Tue, 10 May 2016 19:44:08 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-05-10-R04/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第四单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;trees:0257109cd77173fde404dfe977de0c33&#34;&gt;Trees&lt;/h2&gt;

&lt;p&gt;第四单元的主题是决策树。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:0257109cd77173fde404dfe977de0c33&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;cart-classification-and-regression-trees:0257109cd77173fde404dfe977de0c33&#34;&gt;CART(classification and regression trees)&lt;/h4&gt;

&lt;h6 id=&#34;决策树:0257109cd77173fde404dfe977de0c33&#34;&gt;决策树&lt;/h6&gt;

&lt;p&gt;自变量是决策树上的节点(splits)。但是注意，不是每个自变量都有一个节点；也就是说，有的自变量有多个节点(随着取值的不同，导致因变量的结果也不同)，有的自变量没有节点(对因变量影响很小)。&lt;br /&gt;
因变量是决策树上的叶子/终端(leaves/nodes)。此图上的因变量的取值是0或者1。&lt;br /&gt;
在各个节点，根据各个自变量的取值，最终到达叶子节点，也就得到了因变量的取值。&lt;br /&gt;
注意，决策树的左边，节点的判断语句总是为True／Yes，右边节点的判断语句总是为False／No。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160517-tree-Rplot.png&#34; alt=&#34;tree&#34; /&gt;&lt;br /&gt;
最左的分支表示，如果 LowerCou=lbr 且 Responde=CRI 且 Petition=CIT，那么因变量的取值为0。&lt;br /&gt;
最右的分支表示，如果 LowerCou!=lbr 且 Responde=STA，那么因变量的取值为1。&lt;/p&gt;

&lt;h6 id=&#34;决策树的大小:0257109cd77173fde404dfe977de0c33&#34;&gt;决策树的大小&lt;/h6&gt;

&lt;p&gt;minbucket可以理解为，决策树被节点分割后，每个bucket数据的数量。&lt;br /&gt;
minbucket越大，分组越少，split越少。&lt;br /&gt;
minbucket越小，分组越多，split越多。&lt;/p&gt;

&lt;h6 id=&#34;classification-tree-和-regression-tree:0257109cd77173fde404dfe977de0c33&#34;&gt;Classification tree 和 Regression tree&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;Classification tree analysis is when the predicted outcome is the class to which the data belongs.（简单的讲，预测值是0和1，比如支持还是反对）&lt;/li&gt;
&lt;li&gt;Regression tree analysis is when the predicted outcome can be considered a real number.（简单的讲，预测值是可变的，比如房价等等）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;体现在代码中的话，
如果指定了type = &amp;ldquo;class&amp;rdquo;，那么是 Classification tree。&lt;br /&gt;
如果没有指定type = &amp;ldquo;class&amp;rdquo;，那么是 Regression tree。&lt;/p&gt;

&lt;h4 id=&#34;random-forest:0257109cd77173fde404dfe977de0c33&#34;&gt;Random Forest&lt;/h4&gt;

&lt;p&gt;随机森林，被设计出来用于提高CART的精度。&lt;br /&gt;
和字面意思类似，如果决策树只有一棵树，那么随机森林会创建多个决策树，然后找到效果最好的那一个。&lt;br /&gt;
那么它是如何创建多个决策树的呢，有点复杂。&lt;br /&gt;
它并不是多次调用rpart()，简单的调整几个参数而已。&lt;br /&gt;
每个决策树所用的数据，都只是原数据的随机subset或者说随机子集。&lt;br /&gt;
如果训练集被分成1，2，3，4，5 这五个子集，那么第一次可能选取2，4，5，2，1，第二次可能选取3，5，1，5，2。&lt;/p&gt;

&lt;p&gt;参数nodesize&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;类似于minbucket，每个子集的最小数目。它越小，生成的决策树越大。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数ntree&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;生成多少个决策树。一般几百个就够了。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好消息是，参数的选取，相比CART而言，对结果的影响没有那么大。&lt;/p&gt;

&lt;h4 id=&#34;cross-validation:0257109cd77173fde404dfe977de0c33&#34;&gt;Cross Validation&lt;/h4&gt;

&lt;p&gt;minbucket应该选取什么样的值，来大道最好效果呢？&lt;br /&gt;
我们采用 k-fold cross validation 的方法。&lt;/p&gt;

&lt;p&gt;我们将训练集train分成k份，比如 k=5 的时候，
我们先用1，2，3，4来训练，5用来验证；&lt;br /&gt;
再用1，2，3，5来训练，4用来验证；
再用1，2，4，5来训练，3用来验证。。。
所以模型中创建了很多决策树。
我们测试每个分割方法下，参数每一个可能的取值，计算这个取值对应的预测精度，绘制曲线。&lt;br /&gt;
曲线的X轴是参数的取值，Y轴是预测精度，这样可以很容易找到参数的最佳取值。&lt;/p&gt;

&lt;h6 id=&#34;cp:0257109cd77173fde404dfe977de0c33&#34;&gt;CP&lt;/h6&gt;

&lt;p&gt;像R平方一样，我们也定义了一个概念 cp(complexity parameter) 用来观测效果。&lt;br /&gt;
cp越小，决策树越大(over fitting)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?s = splits&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?lambda = penalty\;error&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?\sum_{leaves}(RSS\;at\;each\;leaf) + lambda*s&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?cp = \frac{lambda}{RSS(no\;splits)}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
cp越大，分母越小，tree越小。&lt;br /&gt;
cp越小，分母越大，tree越大。&lt;/p&gt;

&lt;h3 id=&#34;2-建模和评估:0257109cd77173fde404dfe977de0c33&#34;&gt;2.建模和评估&lt;/h3&gt;

&lt;h4 id=&#34;cart:0257109cd77173fde404dfe977de0c33&#34;&gt;CART&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Install rpart library
install.packages(&amp;quot;rpart&amp;quot;)
library(rpart)
install.packages(&amp;quot;rpart.plot&amp;quot;)
library(rpart.plot)

# CART model
# method=&amp;quot;class&amp;quot; 表示我们创建了一个 classification tree
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=&amp;quot;class&amp;quot;, minbucket=25)

# plot tree
prp(StevensTree)

# Make predictions
# 记得指定 type = &amp;quot;class&amp;quot;
PredictCART = predict(StevensTree, newdata = Test, type = &amp;quot;class&amp;quot;)
table(Test$Reverse, PredictCART)

# ROC curve
library(ROCR)

PredictROC = predict(StevensTree, newdata = Test)
# 注意这里没有指定 type = &amp;quot;class&amp;quot;
# 也就是说，学习得到 classification tree 的模型，但是评估使用 regression tree
# 真是天杀的。。。
# 这个PredictROC 有两列
# 第一列是预测y=0的概率
# 第二列是预测y=1的概率
# 如果比较一下 PredictROC 每行的数据，可以发现这两个概率和为1！那是当然！
# 如果拿 PredictROC 和 PredictCART相比
# 如果 PredictROC[n,2]&amp;gt;0.5，那么PredictCART[n]=1。
# 如果 PredictROC[n,2]&amp;lt;0.5，那么PredictCART[n]=0。
# 所以下面我们只使用第二列

pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(perf)

# AUC
as.numeric(performance(pred, &amp;quot;auc&amp;quot;)@y.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;random-forest-1:0257109cd77173fde404dfe977de0c33&#34;&gt;Random Forest&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;randomForest&amp;quot;)
library(randomForest)

# Build random forest model
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
# Warning message:
# In randomForest.default(m, y, ...) :
#   The response has five or fewer unique values.  Are you sure you want to do regression?

# 如上面的提示消息所示
# randomForest认为因变量的取值很少，不应该用regression
# 但是 random forest 没有 type = &amp;quot;class&amp;quot; 这样的参数
# 所以我们必须确保因变量这一列的取值都是因子
# Convert outcome to factor
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)

# Try again
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )

# Make predictions
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;cross-validation-1:0257109cd77173fde404dfe977de0c33&#34;&gt;Cross Validation&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Install cross-validation packages
install.packages(&amp;quot;caret&amp;quot;)
library(caret)
install.packages(&amp;quot;e1071&amp;quot;)
library(e1071)

# Define cross-validation experiment
numFolds = trainControl( method = &amp;quot;cv&amp;quot;, number = 10 )
cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01)) 

# Perform the cross validation
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = &amp;quot;rpart&amp;quot;, trControl = numFolds, tuneGrid = cpGrid )

# Create a new CART model
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=&amp;quot;class&amp;quot;, cp = 0.18)

# Make predictions
PredictCV = predict(StevensTreeCV, newdata = Test, type = &amp;quot;class&amp;quot;)
table(Test$Reverse, PredictCV)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;参数cp和loss的使用:0257109cd77173fde404dfe977de0c33&#34;&gt;参数cp和loss的使用&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;# Penalty Matrix
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)

# CART model
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method=&amp;quot;class&amp;quot;, cp=0.00005)

prp(ClaimsTree)

# Make predictions
PredictTest = predict(ClaimsTree, newdata = ClaimsTest, type = &amp;quot;class&amp;quot;)
# New CART model with loss matrix
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method=&amp;quot;class&amp;quot;, cp=0.00005, parms=list(loss=PenaltyMatrix))

# Redo predictions and penalty error
PredictTest = predict(ClaimsTree, newdata = ClaimsTest, type = &amp;quot;class&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记03</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-26-R03/</link>
      <pubDate>Tue, 26 Apr 2016 11:52:13 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-26-R03/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第三单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;logistic-regression:2842374fb15231f36230fc5afd744bb4&#34;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;第三单元的主题是指数回归。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:2842374fb15231f36230fc5afd744bb4&#34;&gt;1.理论&lt;/h3&gt;

&lt;h4 id=&#34;指数回归:2842374fb15231f36230fc5afd744bb4&#34;&gt;指数回归&lt;/h4&gt;

&lt;p&gt;指数回归用于因变量y是二进制的情况，也就是说，y的取值只有1或者0。&lt;br /&gt;
y=1的概率：&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?P(y=1)=\frac{1}{1+e^{-{(\beta_0 +\beta_1x_1+\beta_2x_2+\ldots+\beta_nx_n+\epsilon)}}}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;y=1的概率与y＝0的概率的比值：&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=\frac{P(y=1)}{P(y=0)}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=\frac{P(y=1)}{1-P(y=1)}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?Odds=e^{\beta_0 +\beta_1x_1+\beta_2x_2+\ldots+\beta_nx_n+\epsilon}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;混淆矩阵-confusion-matrix:2842374fb15231f36230fc5afd744bb4&#34;&gt;混淆矩阵（confusion matrix）&lt;/h4&gt;

&lt;p&gt;有阈值t，&lt;br /&gt;
如果P(y=1) &amp;gt;=t，则预测y=1。&lt;br /&gt;
如果P(y=1) &amp;lt; t，则预测y=0。&lt;/p&gt;

&lt;p&gt;对于预测结果，我们得到矩阵&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;predict y=0&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;predict y=1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;actual y=0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TN (True  Nagative)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FP (False Positive)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;actual y=1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FN (False Nagative)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TP (True  Positive)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;根据矩阵中的值，我们可以计算指数回归的一些指标：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?accuracy=\frac{TN+TP}{N}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?specificity=\frac{TN}{TN+FP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?sensitivity=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;补充概念：&lt;br /&gt;
适合率&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?precision=\frac{TP}{FP+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
再现率&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?recall=tpr=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
F值（F-measure）&lt;br /&gt;
&lt;img src=&#34;http://latex.codecogs.com/svg.latex?F-measure=\frac{2*precision*recall}{precision+recall}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
F值越高，性能越好&lt;/p&gt;

&lt;h4 id=&#34;roc曲线:2842374fb15231f36230fc5afd744bb4&#34;&gt;ROC曲线&lt;/h4&gt;

&lt;p&gt;ROC曲线 (Receiver Operator Characteristic curve)可以指导我们如何选取阈值t。
y轴的指标是 sensitivity，所以也叫 True positive rate。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?sensitivity=\frac{TP}{FN+TP}&#34; alt=&#34;formula&#34; /&gt;&lt;br /&gt;
x轴的指标是 1-specificity，所以也叫 False positive rate。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?1-sensitivity=\frac{FP}{TN+FP}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;每取一个阈值t，则计算相对应的 TPR 和 FPR，在坐标里标出这个点，就形成ROC曲线。&lt;br /&gt;
&lt;img src=&#34;http://7xrjai.com1.z0.glb.clouddn.com/20160509-ROC.png&#34; alt=&#34;ROC Curve&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;t=0时，我们预测所有的y=1，即TPR=1，FPR=1，对应的坐标是(1,1)   
t=1时，我们预测所有的y=0，即TPR=0，FPR=0，对应的坐标是(0,0)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这就是曲线的两个端点。&lt;/p&gt;

&lt;h4 id=&#34;auc值:2842374fb15231f36230fc5afd744bb4&#34;&gt;AUC值&lt;/h4&gt;

&lt;p&gt;AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。&lt;/p&gt;

&lt;h3 id=&#34;2-建立回归模型:2842374fb15231f36230fc5afd744bb4&#34;&gt;2.建立回归模型&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 建立模型
# Top10作为因变量，其他所有的列都作为自变量
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)

# Top10作为因变量，除了loudness以外的所有列都作为自变量
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-评估:2842374fb15231f36230fc5afd744bb4&#34;&gt;3.评估&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# 预测
testPredict = predict(SongsLog3, newdata=SongsTest, type=&amp;quot;response&amp;quot;)

# 生成混淆矩阵
table(SongsTest$Top10, testPredict &amp;gt;= 0.45)

# 生成ROC曲线
library(ROCR)
pred = prediction(testPredict, test$violator)
perf = performance(pred, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(perf)

# 加点颜色和坐标点
plot(perf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))

# 计算AUC值
as.numeric(performance(pred, &amp;quot;auc&amp;quot;)@y.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;附录a-分割train和test的方法一:2842374fb15231f36230fc5afd744bb4&#34;&gt;附录A 分割train和test的方法一&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(caTools)
set.seed(144)

split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
# 特别注意：每次运行出来的结果是不一样的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(caTools)
set.seed(144)

split = sample(1:nrow(data), size=0.7 * nrow(data))
train = data[split,]
test = data[-split,]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;附录b-补充缺失数据:2842374fb15231f36230fc5afd744bb4&#34;&gt;附录B 补充缺失数据&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), &amp;quot;not.fully.paid&amp;quot;)
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记02</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-23-R02/</link>
      <pubDate>Sat, 23 Apr 2016 15:19:39 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-23-R02/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第二单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;linear-regression:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;Linear Regression&lt;/h2&gt;

&lt;p&gt;第二单元的主题是线性回归。&lt;/p&gt;

&lt;h3 id=&#34;1-理论:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;1.理论&lt;/h3&gt;

&lt;p&gt;一元线性回归公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?y^i=\beta_0+\beta_1x^i+\epsilon^i&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中x是自变量independent variable，y是因变量dependent variable。&lt;br /&gt;
beta是相关系数coefficient，epsilon是误差error。&lt;/p&gt;

&lt;p&gt;为了判断线性回归的效果，我们有如下检验标准：&lt;/p&gt;

&lt;p&gt;1.SSE（sum of squared errors）&lt;br /&gt;
注意这里的误差是实际值相对于预测值的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?SSE = \sum_{i=1}^{n}\epsilon_i^2&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.SST （total sum of square）&lt;br /&gt;
公式同上。但这里的误差是实际值相对于baseline的。baseline是因变量的平均值。&lt;br /&gt;
所以有 0 &amp;lt;= SSE &amp;lt;= SST 。&lt;/p&gt;

&lt;p&gt;3.RMSE（root mean square error）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?RMSE = \sqrt\frac{SSE}{n}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.R平方&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?R^2 = 1 - \frac{SSE}{SST}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R平方越接近1越好。&lt;/p&gt;

&lt;p&gt;多元线性回归公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?y^i = \beta_0 + \beta_1x_1^i + \beta_2x_2^i + \ldots + \beta_nx_n^i + \epsilon^i&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;所有数据分析，都要经历 training－test－predict 这三个过程。
在接下来的例子中，我们介绍 建模－评估 这前两个过程。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;补充一个relative error的公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?relative\;error =  \frac{observed\;value - estimated\,value}{observed\;value}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;2-0-事前整理:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;2.0 事前整理&lt;/h5&gt;

&lt;p&gt;2.0.1 去除空值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 如果数据中包含空值
DF ＝ na.omit(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.0.2 reference level&lt;br /&gt;
有些列时字符型的，它们无法进行计算。&lt;br /&gt;
如果某列的因子不算多，我们可以把这一列变换成多个可以用于计算的列。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 假设DF$colr有因子 &amp;quot;Red&amp;quot;4次, &amp;quot;Blue&amp;quot;3次, &amp;quot;Yellow&amp;quot;2次
DF$colr = relevel(DF$colr, &amp;quot;red&amp;quot;)

# 效果是，DF$colr 这一列不见了
# 增加了两列 DF$colrBlue 和 DF$colrYellow
# 原先 DF$colr == &amp;quot;Red&amp;quot; 的那些行，它们 colrBlue 和 colrYellow 的值都是0
# 原先 DF$colr == &amp;quot;Blue&amp;quot; 的那些行，它们 colrBlue=1, colrYellow=0
# 原先 DF$colr == &amp;quot;Yellow&amp;quot; 的那些行，它们 colrBlue=0, colrYellow=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-建立回归模型:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;2.建立回归模型&lt;/h3&gt;

&lt;p&gt;建模使用lm()函数。&lt;br /&gt;
DF是保存学习数据的data.frame。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = lm(y ~ x1 + x2 + ... +xn, data = DF)
# y不要写成 DF$y
# x1也不要写成 DF$x1
# 否则，后面做预测predict()的时候，DFTest代入会报warning

# 除了y列以外所有列
model = lm(y ~ ., data = DF)

# 误差 model$residuals
SSE = sum(model$residuals^2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;随便看个结果吧&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; summary(model)

Call:
lm(formula = Price ~ HarvestRain + WinterRain, data = wine)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.0933 -0.3222 -0.1012  0.3871  1.1877 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  7.865e+00  6.616e-01  11.888 4.76e-11 ***
HarvestRain -4.971e-03  1.601e-03  -3.105  0.00516 ** 
WinterRain  -9.848e-05  9.007e-04  -0.109  0.91392    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5611 on 22 degrees of freedom
Multiple R-squared:  0.3177,    Adjusted R-squared:  0.2557 
F-statistic: 5.122 on 2 and 22 DF,  p-value: 0.01492
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Call表示建模使用的语句。&lt;br /&gt;
Residuals表示误差。&lt;br /&gt;
Coefficients表示系数，就是公式里面的beta。&lt;br /&gt;
Estimate的第一行是常数beta0，第二行是第一个自变量的系数beta1，第三行是第二个自变量的系数beta2，后面类推。&lt;br /&gt;
t value越大越好&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/svg.latex?{t\,value} = \frac{Estimate}{Std. Error}&#34; alt=&#34;formula&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Pr(&amp;gt;|t|) 和t value相反，越小越好。&lt;br /&gt;
最后一列星星越多越好。&lt;br /&gt;
三短横下面这行解释了星星的含义。&lt;br /&gt;
Multiple R-squared就是R平方，越接近1越准确。&lt;/p&gt;

&lt;h3 id=&#34;3-评估:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;3.评估&lt;/h3&gt;

&lt;p&gt;对于刚过简历的模型，我们使用测试数据来评估一下准确度。&lt;br /&gt;
model就是上文建立的模型。&lt;br /&gt;
DFTest是测试数据，它的结构和上文的DF一样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;predict = predict(model, newdata = DFTest)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个命令的返回值是 DFTest$Price 的&lt;strong&gt;预测&lt;/strong&gt;结果。你可以跟 DFTest$Price 的&lt;strong&gt;实际&lt;/strong&gt;结果相比较，计算SSE、RMSE、R平方等等来衡量对测试数据的预测的准确性。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SSE = sum( (DFTest$Price - predict)^2 )
SST = sum( (DFTest$Price - mean(DF$Price)^2 )
R2 = 1 - SSE/SST
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-correlation:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;4.Correlation&lt;/h3&gt;

&lt;p&gt;线性相关性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cor(var1, var2)
# 也可以考察整个DF中，每两列的线性相关性
cor(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回值是斜率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建立线性回归模型的时候，应该去掉相关性比较高的列。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;补充知识a-棒球统计术语:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识A－棒球统计术语&lt;/h3&gt;

&lt;p&gt;完全不懂棒球啊，一开始摸不着头脑。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;缩写&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;原文&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;中文&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Run Scores&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;跑分，得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;RA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Run Allowed&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失分，对手得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OBP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;On-Base Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;上垒率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OOBP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent On-Base Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;对手上垒率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SLG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Slugging Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;长打率，击中率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OSLG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent Slugging Percentage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;对手长打率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Batting Avarage&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;平均成功率&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;补充知识b-篮球统计术语:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识B－篮球统计术语&lt;/h3&gt;

&lt;p&gt;年轻时看NBA，好歹知道一点。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;缩写&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;原文&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;中文&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;PTS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;oppPTS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opponent Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失分，对手得分&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Field Goals (success)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FGA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Field Goals Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X2P&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2 Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2分球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X2PA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2 Points Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2分球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X3P&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3 Points&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3分球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;X3PA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3 Points Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3分球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Free Throw&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;罚球进球数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FTA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Free Throw Attempted&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;发球出手次数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ORB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Offensive Rebounds&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;前场篮板，进攻篮板&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;DRB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Defensive Rebounds&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;后场篮板，防守篮板&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;AST&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Assists&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;助攻&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;STL&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steals&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;抢断&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BLK&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Blocks&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;盖帽&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;TOV&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Turnovers&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;失误&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;注：X2P列，原始数据列名是2P。由于R不支持数字开头的列名／变量，读取CSV文件的时候，会在原列名2P前加个X，从而变成 X2P。&lt;/p&gt;

&lt;h3 id=&#34;补充知识c-滞后序列:efa3f4364b4acd6454b77ef89af35a47&#34;&gt;补充知识C－滞后序列&lt;/h3&gt;

&lt;p&gt;函数lag，用于生成滞后/偏移序列？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lag(x, k = 1, ...)
# k &amp;lt; 0, previous observations   
# k &amp;gt; 0, future observations
# na.pad=TRUE, add missing values
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>R语言笔记01</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-18-R01/</link>
      <pubDate>Mon, 18 Apr 2016 19:55:25 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-18-R01/</guid>
      <description>

&lt;p&gt;MIT课程 &lt;a href=&#34;https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/info&#34;&gt;15.071x&lt;/a&gt; 第一单元的学习记录。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;r语言入门:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;R语言入门&lt;/h2&gt;

&lt;p&gt;R语言入门只讲了一些常用的操作。相对于动辄花一本书来讲这些，真是相当简约。但其实足够了，其他操作，需要的时候再查嘛。&lt;br /&gt;
数据分析的四要素：data、models、decisions、value&lt;/p&gt;

&lt;h3 id=&#34;简单使用:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;简单使用&lt;/h3&gt;

&lt;p&gt;帮助：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;?func
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示当前的临时变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取／设置当前目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getwd()
setwd()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示当前文件夹下的文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据结构:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;向量概念：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;所有的操作都是对向量的每个元素实施的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;data.frame：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;observation：行
variable   ：列，data.frame是按照列存储的
rbind()合并两个data.frame
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;序列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;自动生成序列seq()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;获取数据:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;获取数据&lt;/h3&gt;

&lt;p&gt;读取CSV文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF = read.csv(&amp;quot;file_path&amp;quot;)
# 返回值是data.frame
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看DF的基本信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;str(DF)
# DF的结构信息。行和列的数目，列名、列的类型、列的数据举例。

summary(DF)
# 每列的最大值、最小值、中位数、平均数、1/4值、3/4值，以及是否包含空值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写入CSV文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;write.csv(DF, &amp;quot;file_path&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从内存中删除变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;数据操作:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;数据操作&lt;/h3&gt;

&lt;p&gt;选取一部分数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subset( DF, 条件1 &amp;amp; 条件2 ｜ 条件3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照列名选取3列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF[c(var1, var2, var3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选取1，3，5列：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DF[c(1, 3, 5)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算平均值和标准差：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mean(DF$var)
sd(DF$var)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回最大值／最小值的位置(index)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;which.max(DF$var)
which.min(DF$var)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;返回行数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nrow(DF)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;绘图:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;绘图&lt;/h3&gt;

&lt;p&gt;直方图，反映&lt;strong&gt;一列&lt;/strong&gt;数据的分布情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hist(DF$var, xlim = c(1, 100), breaks = 100)
# xlim 限定范围
# breaks x轴的精确度。注意是针对原始数据的，不是对限定后的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;箱型图：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;boxplot(DF$var1 ~ DF$var2, xlab = &amp;quot;x-label&amp;quot;, ylab = &amp;quot;y-label&amp;quot;, main = &amp;quot;title&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;点阵图：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plot(......, col = &amp;quot;red&amp;quot;)
line(......, col = &amp;quot;blue&amp;quot;) # 在原先的基础上再加一条

函数jitter() # 对于有很多重合的点阵图，先用jitter偏移一点，这样看上去效果好很多
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其他共通的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col = &amp;quot;red&amp;quot;
type = &amp;quot;line&amp;quot; # 可以指定1，2，3，4，5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;聚合:86ba9dc4beedcb3ca4aa6515cf113aed&#34;&gt;聚合&lt;/h3&gt;

&lt;p&gt;分组：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table(DF$var1)
# var1列中，每种数据的数量的统计
table(DF$var1, DF$var2)
# var1和var2列中，每种数据的数量的交叉统计
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分组计算：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tapply(DF$var1, DF$var2, func)
# DF$var1, 原始数据
# DF$var2, 分组依据
# func, 要应用的函数
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>data.table 教程5（校对中）</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-08-datatable5/</link>
      <pubDate>Fri, 08 Apr 2016 13:00:57 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-08-datatable5/</guid>
      <description>

&lt;p&gt;目录：&lt;br /&gt;
1) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-13-datatable1/&#34;&gt;data.table 介绍&lt;/a&gt;&lt;br /&gt;
2) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-21-datatable2/&#34;&gt;语义引用&lt;/a&gt;&lt;br /&gt;
3) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-22-datatable3/&#34;&gt;主键、基于快速二分法搜索的subset&lt;/a&gt;&lt;br /&gt;
4) &lt;a href=&#34;http://youngspring1.github.io/post/2016-04-02-datatable4/&#34;&gt;二级索引和自动索引&lt;/a&gt;&lt;br /&gt;
5) 有效的数据拆分和合并&lt;/p&gt;

&lt;p&gt;原文地址：&lt;br /&gt;
1) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-intro-vignette.html&#34;&gt;Introduction to data.table&lt;/a&gt;&lt;br /&gt;
2) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-reference-semantics.html&#34;&gt;Reference semantics&lt;/a&gt;&lt;br /&gt;
3) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-keys-fast-subset.html&#34;&gt;Keys and fast binary search based subsets&lt;/a&gt;&lt;br /&gt;
4) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-secondary-indices-and-auto-indexing.html&#34;&gt;Secondary indices and auto indexing&lt;/a&gt;&lt;br /&gt;
5) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-reshape.html&#34;&gt;Efficient reshaping using data.tables&lt;/a&gt;&lt;br /&gt;
6) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-faq.html&#34;&gt;Frequently asked questions&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;有效的数据拆分和合并:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;有效的数据拆分和合并&lt;/h1&gt;

&lt;p&gt;这一讲我们学习reshaping函数 melt 和 dcast 原本的用法，以及从R语言 v1.9.6版以后，函数 melt 和 dcast 新扩展的功能（它们能操作多个列）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;数据:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;数据&lt;/h2&gt;

&lt;p&gt;我们在讲解的时候直接加载数据。&lt;/p&gt;

&lt;h2 id=&#34;介绍:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;介绍&lt;/h2&gt;

&lt;p&gt;data.table的函数melt 和 dcast 是增强包&lt;a href=&#34;https://cran.r-project.org/web/packages/reshape2/index.html&#34;&gt;reshape2&lt;/a&gt;里同名函数的扩展。&lt;br /&gt;
在这一讲，我们会：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先，简单看一下原先的函数 melt 和 dcast，它们是如何reshaping一个data.table。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后，了解一下当前的功能是如何变得冗长而且低效。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后，学习一下改进之后的函数 melt 和 dcast 如何同时处理多个列。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;扩展后的功能符合data.table的设计哲学：运行高效，语法简明。&lt;/p&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;从R语言 v1.9.6版以后，你再也不需要加载增强包 reshape2 了，只需要加载 data.table。如果你已经加载了 reshape2 来处理矩阵或者data.frame，那么一定要确保在这之后再加载 data.table。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;1-原生的melt-dcast:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;1.原生的melt／dcast&lt;/h2&gt;

&lt;h4 id=&#34;a-函数melt:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;a) 函数melt&lt;/h4&gt;

&lt;p&gt;假设我们有下面这样的data.table：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT = fread(&amp;quot;melt_default.csv&amp;quot;)
DT
#    family_id age_mother dob_child1 dob_child2 dob_child3
# 1:         1         30 1998-11-26 2000-01-29         NA
# 2:         2         27 1996-06-22         NA         NA
# 3:         3         26 2002-07-11 2004-04-05 2007-09-02
# 4:         4         32 2004-10-10 2009-08-27 2012-07-21
# 5:         5         29 2000-12-05 2005-02-28         NA
## dob stands for date of birth.

str(DT)
# Classes &#39;data.table&#39; and &#39;data.frame&#39;:    5 obs. of  5 variables:
#  $ family_id : int  1 2 3 4 5
#  $ age_mother: int  30 27 26 32 29
#  $ dob_child1: chr  &amp;quot;1998-11-26&amp;quot; &amp;quot;1996-06-22&amp;quot; &amp;quot;2002-07-11&amp;quot; &amp;quot;2004-10-10&amp;quot; ...
#  $ dob_child2: chr  &amp;quot;2000-01-29&amp;quot; NA &amp;quot;2004-04-05&amp;quot; &amp;quot;2009-08-27&amp;quot; ...
#  $ dob_child3: chr  NA NA &amp;quot;2007-09-02&amp;quot; &amp;quot;2012-07-21&amp;quot; ...
#  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;－转化DT，使得每个小孩的出生信息都独占一条数据&lt;br /&gt;
我们可以对函数 melt() 指定参数 id.vars 和 measure.vars 来实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT.m1 = melt(DT, id.vars = c(&amp;quot;family_id&amp;quot;, &amp;quot;age_mother&amp;quot;),
        measure.vars = c(&amp;quot;dob_child1&amp;quot;, &amp;quot;dob_child2&amp;quot;, &amp;quot;dob_child3&amp;quot;))
DT.m1
#     family_id age_mother   variable      value
#  1:         1         30 dob_child1 1998-11-26
#  2:         2         27 dob_child1 1996-06-22
#  3:         3         26 dob_child1 2002-07-11
#  4:         4         32 dob_child1 2004-10-10
#  5:         5         29 dob_child1 2000-12-05
#  6:         1         30 dob_child2 2000-01-29
#  7:         2         27 dob_child2         NA
#  8:         3         26 dob_child2 2004-04-05
#  9:         4         32 dob_child2 2009-08-27
# 10:         5         29 dob_child2 2005-02-28
# 11:         1         30 dob_child3         NA
# 12:         2         27 dob_child3         NA
# 13:         3         26 dob_child3 2007-09-02
# 14:         4         32 dob_child3 2012-07-21
# 15:         5         29 dob_child3         NA
str(DT.m1)
# Classes &#39;data.table&#39; and &#39;data.frame&#39;:    15 obs. of  4 variables:
#  $ family_id : int  1 2 3 4 5 1 2 3 4 5 ...
#  $ age_mother: int  30 27 26 32 29 30 27 26 32 29 ...
#  $ variable  : Factor w/ 3 levels &amp;quot;dob_child1&amp;quot;,&amp;quot;dob_child2&amp;quot;,..: 1 1 1 1 1 2 2 2 2 2 ...
#  $ value     : chr  &amp;quot;1998-11-26&amp;quot; &amp;quot;1996-06-22&amp;quot; &amp;quot;2002-07-11&amp;quot; &amp;quot;2004-10-10&amp;quot; ...
#  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;参数 measure.vars 指定了想要拆分（或合并）的列。&lt;/li&gt;
&lt;li&gt;我们也可以指定索引而不是列名。&lt;/li&gt;
&lt;li&gt;默认的，variable列是 factor（因子）类型的。如果你想返回一个字符型的向量，可以将参数 variable.factor 设为 FALSE。参数 variable.factor 是data.table的函数melt() 里独有的，增强包reshape2 里面没有这个参数。&lt;/li&gt;
&lt;li&gt;默认的，转化果的列被自动命名为 variable 和 value。&lt;/li&gt;
&lt;li&gt;在结果里，函数melt() 保持了原来列的属性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;－分别将 variable列和 value列重命名为 child 和 dob&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT.m1 = melt(DT, measure.vars = c(&amp;quot;dob_child1&amp;quot;, &amp;quot;dob_child2&amp;quot;, &amp;quot;dob_child3&amp;quot;),
           variable.name = &amp;quot;child&amp;quot;, value.name = &amp;quot;dob&amp;quot;)
DT.m1
#     family_id age_mother      child        dob
#  1:         1         30 dob_child1 1998-11-26
#  2:         2         27 dob_child1 1996-06-22
#  3:         3         26 dob_child1 2002-07-11
#  4:         4         32 dob_child1 2004-10-10
#  5:         5         29 dob_child1 2000-12-05
#  6:         1         30 dob_child2 2000-01-29
#  7:         2         27 dob_child2         NA
#  8:         3         26 dob_child2 2004-04-05
#  9:         4         32 dob_child2 2009-08-27
# 10:         5         29 dob_child2 2005-02-28
# 11:         1         30 dob_child3         NA
# 12:         2         27 dob_child3         NA
# 13:         3         26 dob_child3 2007-09-02
# 14:         4         32 dob_child3 2012-07-21
# 15:         5         29 dob_child3         NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;默认的，参数id.vars 或 measure.vars 中的一个省略了，剩余的列自动被赋值给省略的那个参数。&lt;/li&gt;
&lt;li&gt;如果参数id.vars 和 measure.vars 都没有指定，所有不是numberic／integer／logical的列都会被赋值给 id.vars。另外，系统还会输出一条警告消息，提示那些列被认为是 id.vars。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-函数cast:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;b) 函数cast&lt;/h4&gt;

&lt;p&gt;在前面一节，我们知道如何分拆数据。这一节，我们学习相反的操作。&lt;br /&gt;
－如何将刚刚分拆的 DT.m 还原成 DT&lt;br /&gt;
也就是，我们想把每个家庭／母亲的所有小孩，都合并到同一行里。我们可以像下面这样使用函数 dcast()。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dcast(DT.m1, family_id + age_mother ~ child, value.var = &amp;quot;dob&amp;quot;)
#    family_id age_mother dob_child1 dob_child2 dob_child3
# 1:         1         30 1998-11-26 2000-01-29         NA
# 2:         2         27 1996-06-22         NA         NA
# 3:         3         26 2002-07-11 2004-04-05 2007-09-02
# 4:         4         32 2004-10-10 2009-08-27 2012-07-21
# 5:         5         29 2000-12-05 2005-02-28         NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;函数 dcast() 使用了操作符“~”，左边是作为 id.vars 的列，右边是作为 measure.vars 的列。&lt;/li&gt;
&lt;li&gt;参数 value.var 指定了需要被分拆扩张的列。&lt;/li&gt;
&lt;li&gt;函数 dcast() 也会在结果中尽量保持原来的属性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;－对于 DT.m，如何知道每个家庭有几个小孩&lt;br /&gt;
可以给函数 dcast() 的参数 fun.aggregate 传递一个函数。当操作符“~”不方便指定列名的时候，这个功能特别有用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dcast(DT.m1, family_id ~ ., fun.agg = function(x) sum(!is.na(x)), value.var = &amp;quot;dob&amp;quot;)
#    family_id .
# 1:         1 2
# 2:         2 1
# 3:         3 3
# 4:         4 3
# 5:         5 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入 ?dcast 可以查看其他参数和例子的说明。&lt;/p&gt;

&lt;h2 id=&#34;2-原生的melt-dcast的局限:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;2.原生的melt／dcast的局限&lt;/h2&gt;

&lt;p&gt;到目前为止，我们学习了函数 melt 和 dcast 的功能，它们是基于增强包 reshape2 的。但是因为使用了data.table的内部机制（快速排序，二分法搜索等），所以能有效地对data.table实行。&lt;br /&gt;
然而，也有一些情况，我们想做的操作无法写得很简洁。比如，考虑下面这个data.table：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT = fread(&amp;quot;melt_enhanced.csv&amp;quot;)
DT
#    family_id age_mother dob_child1 dob_child2 dob_child3 gender_child1 gender_child2 gender_child3
# 1:         1         30 1998-11-26 2000-01-29         NA             1             2            NA
# 2:         2         27 1996-06-22         NA         NA             2            NA            NA
# 3:         3         26 2002-07-11 2004-04-05 2007-09-02             2             2             1
# 4:         4         32 2004-10-10 2009-08-27 2012-07-21             1             1             1
# 5:         5         29 2000-12-05 2005-02-28         NA             2             1            NA
## 1 = female, 2 = male
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你想用我们到目前为止学过的知识，将每个孩子的 dob 和 gender 合并到一行中，得这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT.m1 = melt(DT, id = c(&amp;quot;family_id&amp;quot;, &amp;quot;age_mother&amp;quot;))
# Warning in melt.data.table(DT, id = c(&amp;quot;family_id&amp;quot;, &amp;quot;age_mother&amp;quot;)): &#39;measure.vars&#39; [dob_child1,
# dob_child2, dob_child3, gender_child1, gender_child2, gender_child3] are not all of the same
# type. By order of hierarchy, the molten data value column will be of type &#39;character&#39;. All measure
# variables not of type &#39;character&#39; will be coerced to. Check DETAILS in ?melt.data.table for more on
# coercion.
DT.m1[, c(&amp;quot;variable&amp;quot;, &amp;quot;child&amp;quot;) := tstrsplit(variable, &amp;quot;_&amp;quot;, fixed = TRUE)]
DT.c1 = dcast(DT.m1, family_id + age_mother + child ~ variable, value.var = &amp;quot;value&amp;quot;)
DT.c1
#     family_id age_mother  child        dob gender
#  1:         1         30 child1 1998-11-26      1
#  2:         1         30 child2 2000-01-29      2
#  3:         1         30 child3         NA     NA
#  4:         2         27 child1 1996-06-22      2
#  5:         2         27 child2         NA     NA
#  6:         2         27 child3         NA     NA
#  7:         3         26 child1 2002-07-11      2
#  8:         3         26 child2 2004-04-05      2
#  9:         3         26 child3 2007-09-02      1
# 10:         4         32 child1 2004-10-10      1
# 11:         4         32 child2 2009-08-27      1
# 12:         4         32 child3 2012-07-21      1
# 13:         5         29 child1 2000-12-05      2
# 14:         5         29 child2 2005-02-28      1
# 15:         5         29 child3         NA     NA

str(DT.c1) ## gender column is character type now!
# Classes &#39;data.table&#39; and &#39;data.frame&#39;:    15 obs. of  5 variables:
#  $ family_id : int  1 1 1 2 2 2 3 3 3 4 ...
#  $ age_mother: int  30 30 30 27 27 27 26 26 26 32 ...
#  $ child     : chr  &amp;quot;child1&amp;quot; &amp;quot;child2&amp;quot; &amp;quot;child3&amp;quot; &amp;quot;child1&amp;quot; ...
#  $ dob       : chr  &amp;quot;1998-11-26&amp;quot; &amp;quot;2000-01-29&amp;quot; NA &amp;quot;1996-06-22&amp;quot; ...
#  $ gender    : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; NA &amp;quot;2&amp;quot; ...
#  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt; 
#  - attr(*, &amp;quot;sorted&amp;quot;)= chr  &amp;quot;family_id&amp;quot; &amp;quot;age_mother&amp;quot; &amp;quot;child&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我们想做的是，分别将每个孩子的 dob 和 gender 合并到一行。但是我们先把所有的东西都拆分开了，再将它们合并。很容易看出，这太过迂回和低效了。
类似的，想想你的壁橱里有4架子的衣服，你想把第1架和第2架的衣服全都放到第1架上，把第3架和第4架的衣服全都放到第3架上。我们刚刚做的事情，就像把4架衣服都放一起，再分开放到第1架和第3架上！&lt;/li&gt;
&lt;li&gt;需要被整合的列可能是不同的类型，在这个例子里面，是字符型和整型。使用函数melt 的时候，这些列被硬塞到结果里面，正如str(DT.c1)的警告消息所提示的，gender列被转化成了字符型。&lt;/li&gt;
&lt;li&gt;我们将variable拆分成了两列，因此额外多了一列，这样做的目的真是非常模糊。我们这么做是因为下一步我们需要转化这一列。&lt;/li&gt;
&lt;li&gt;最后，我们整合了数据。但是问题是我们引入很多操作。特别是，必须要计算等式中变量的顺序，代价太大。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;事实上，base::reshape 有简单的写法来实现这个操作。它非常有用，而且经常被低估。你应该试试！&lt;/p&gt;

&lt;h2 id=&#34;3-增强的新功能:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;3.增强的新功能&lt;/h2&gt;

&lt;h4 id=&#34;a-增强的melt:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;a) 增强的melt&lt;/h4&gt;

&lt;p&gt;既然我们希望简单地实现同样的操作，我们实现了一个额外的功能，这样就可以同时操作多个列。&lt;br /&gt;
－用函数melt 同时拆分多个列&lt;br /&gt;
这个办法很简单。我们给参数 measure.vars 传递一个列表，这个列表的每个元素包含需要被合并的列。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;colA = paste(&amp;quot;dob_child&amp;quot;, 1:3, sep = &amp;quot;&amp;quot;)
colB = paste(&amp;quot;gender_child&amp;quot;, 1:3, sep = &amp;quot;&amp;quot;)
DT.m2 = melt(DT, measure = list(colA, colB), value.name = c(&amp;quot;dob&amp;quot;, &amp;quot;gender&amp;quot;))
DT.m2
#     family_id age_mother variable        dob gender
#  1:         1         30        1 1998-11-26      1
#  2:         2         27        1 1996-06-22      2
#  3:         3         26        1 2002-07-11      2
#  4:         4         32        1 2004-10-10      1
#  5:         5         29        1 2000-12-05      2
#  6:         1         30        2 2000-01-29      2
#  7:         2         27        2         NA     NA
#  8:         3         26        2 2004-04-05      2
#  9:         4         32        2 2009-08-27      1
# 10:         5         29        2 2005-02-28      1
# 11:         1         30        3         NA     NA
# 12:         2         27        3         NA     NA
# 13:         3         26        3 2007-09-02      1
# 14:         4         32        3 2012-07-21      1
# 15:         5         29        3         NA     NA

str(DT.m2) ## col type is preserved
# Classes &#39;data.table&#39; and &#39;data.frame&#39;:    15 obs. of  5 variables:
#  $ family_id : int  1 2 3 4 5 1 2 3 4 5 ...
#  $ age_mother: int  30 27 26 32 29 30 27 26 32 29 ...
#  $ variable  : Factor w/ 3 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;: 1 1 1 1 1 2 2 2 2 2 ...
#  $ dob       : chr  &amp;quot;1998-11-26&amp;quot; &amp;quot;1996-06-22&amp;quot; &amp;quot;2002-07-11&amp;quot; &amp;quot;2004-10-10&amp;quot; ...
#  $ gender    : int  1 2 2 1 2 2 NA 2 1 1 ...
#  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;－函数 patterns()&lt;br /&gt;
通常，我们想整合的这些列的列名都有共通的格式。我们可以用函数patterns()指定正则表达式，让语法更简洁。上面的操作还可以这样写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DT.m2 = melt(DT, measure = patterns(&amp;quot;^dob&amp;quot;, &amp;quot;^gender&amp;quot;), value.name = c(&amp;quot;dob&amp;quot;, &amp;quot;gender&amp;quot;))
DT.m2
#     family_id age_mother variable        dob gender
#  1:         1         30        1 1998-11-26      1
#  2:         2         27        1 1996-06-22      2
#  3:         3         26        1 2002-07-11      2
#  4:         4         32        1 2004-10-10      1
#  5:         5         29        1 2000-12-05      2
#  6:         1         30        2 2000-01-29      2
#  7:         2         27        2         NA     NA
#  8:         3         26        2 2004-04-05      2
#  9:         4         32        2 2009-08-27      1
# 10:         5         29        2 2005-02-28      1
# 11:         1         30        3         NA     NA
# 12:         2         27        3         NA     NA
# 13:         3         26        3 2007-09-02      1
# 14:         4         32        3 2012-07-21      1
# 15:         5         29        3         NA     NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是这样！&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果需要，我们可以去掉 variable列。&lt;/li&gt;
&lt;li&gt;这个功能是用C实现的，因此效率高，节省内存，而且简洁。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-增强的dcast:8a0e17190d0dbfa1c06f28f3c1a1fd66&#34;&gt;b) 增强的dcast&lt;/h4&gt;

&lt;p&gt;非常好！现在我们可以同时拆分多个列了。现在我们如何将上面的 DT.m2 再恢复成原来的样子呢？&lt;br /&gt;
如果我们使用原生的函数dcast()，我们需要做两次，然后将结果合并在一起。但是这样做太麻烦，一点也不简洁和有效。&lt;br /&gt;
－同时合并多个 value.vars&lt;br /&gt;
我们可以对函数dcast()指定多个 value.var参数，这样操作就在内部进行，而且高效。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## new &#39;cast&#39; functionality - multiple value.vars
DT.c2 = dcast(DT.m2, family_id + age_mother ~ variable, value.var = c(&amp;quot;dob&amp;quot;, &amp;quot;gender&amp;quot;))
DT.c2
#    family_id age_mother      dob_1      dob_2      dob_3 gender_1 gender_2 gender_3
# 1:         1         30 1998-11-26 2000-01-29         NA        1        2       NA
# 2:         2         27 1996-06-22         NA         NA        2       NA       NA
# 3:         3         26 2002-07-11 2004-04-05 2007-09-02        2        2        1
# 4:         4         32 2004-10-10 2009-08-27 2012-07-21        1        1        1
# 5:         5         29 2000-12-05 2005-02-28         NA        2        1       NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在结果中，原先的属性会尽量保持。&lt;/li&gt;
&lt;li&gt;所有的事情都在内部高效处理。快速并且节省内存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参数fun.aggregate可以指定多个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;你可以给函数dcast()的参数fun.aggregate可以指定多个函数。详细内容请执行 ?dcast 来查看示例。
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>转：任务切换有害论</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-07-human-task-switches/</link>
      <pubDate>Thu, 07 Apr 2016 22:31:55 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-07-human-task-switches/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;书摘，来自《软件随想录》，Joel Spolsky&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在管理一个程序团队时，第一件要学的事就是任务配置(task allocation)要正确。「任务配置」只是把事情分给大家做的夸大说法。用希伯来文的普通话来说就是「倒文件」(因为你会把文件倒在某人身上)。有些事情做得对会得到不可思议的生产力利益，决定哪些文件要倒在谁身上就是其中之一。反过来没做好的话可能就会陷入麻烦的状况，没有人能做好何任何事情而且大家都抱怨「在这里什么事都做不起来。」&lt;/p&gt;

&lt;p&gt;由于这是个针对程序员的网站，我要拿个程序设计问题让你的脑袋动一动暖暖身。&lt;/p&gt;

&lt;p&gt;假设你有A和B两件运算要做。每一件都需10秒的CPU时间。现在你有一颗CPU，为了简化问题，所以工作序列中没有其他东西。&lt;/p&gt;

&lt;p&gt;在我们的CPU中可以选择是否用多工处理。所以你可以先做好一件再做另一件。&lt;/p&gt;

&lt;h5 id=&#34;循序处理:adf6769e1c0a1dd908086b693a0d7412&#34;&gt;循序处理&lt;/h5&gt;

&lt;p&gt;运算A&lt;br /&gt;
1   2   3   4   5   6   7   8   9   10&lt;br /&gt;
运算B&lt;br /&gt;
11  12  13  14  15  16  17  18  19  20&lt;/p&gt;

&lt;p&gt;也可以使用多工方式。如果用多工的话可以假设这颗特别的CPU每个工作每次可以执行一秒，而且工作切换完全不花时间。&lt;/p&gt;

&lt;h5 id=&#34;多工处理:adf6769e1c0a1dd908086b693a0d7412&#34;&gt;多工处理&lt;/h5&gt;

&lt;p&gt;运算A&lt;br /&gt;
1   3   5   7   9   11  13  15  17  19&lt;br /&gt;
运算B&lt;br /&gt;
2   4   6   8   10  12  14  16  18  20&lt;/p&gt;

&lt;p&gt;你会选哪一种方式呢？大部份人的直觉反应都认为多工比较好。不管哪一种状况，都得等20秒才能两件运算都完成。不过可以想想单就各件运算来说要多久才有结果。&lt;/p&gt;

&lt;p&gt;在两种状况下，运算B(标成蓝色)都要20秒才得到结果。不过运算A的结果在多工时需要19秒。可是循序时就只要10秒就好了。&lt;/p&gt;

&lt;p&gt;换句话来说在这个安排好的例子中，循序处理的每件运算的平均时间比多工处理少(15秒对19.5秒)。(事实上这例子也并不是真的那么假 - 它是源于Jared在工作上必须解决的一个真实问题。)&lt;/p&gt;

&lt;p&gt;方法  运算A花的时间 运算B花的时间 平均&lt;br /&gt;
循序处理    10秒 20秒 15&lt;br /&gt;
多工处理    19秒 20秒 19.5&lt;/p&gt;

&lt;p&gt;我刚刚说过「工作切换完全不花时间」。其实在真的CPU中工作切换是需要一点点时间的，基本上要足够储存CPU暂存器的状态并载入其他工作的CPU暂存器。实际上这短到几乎可以忽略。不过为了让生活更多乐趣，让我们假设工作切换需要半秒。现在情况变得更糟了：&lt;/p&gt;

&lt;p&gt;方法  运算A花的时间 运算B花的时间 平均&lt;br /&gt;
循序处理    10秒 20秒 + 1次工作切换 = 20.5秒    15.25&lt;br /&gt;
多工处理    19秒 + 18次工作切换 = 28秒 20秒 + 19次工作切换 = 29.5秒   28.75&lt;/p&gt;

&lt;p&gt;现在呢，虽然我知道这有点蠢，不过就算为了让我高兴一下，想想如果工作切换需要一分钟拿如何？&lt;/p&gt;

&lt;p&gt;方法  运算A花的时间 运算B花的时间 平均&lt;br /&gt;
循序处理    10秒 20秒 + 1次工作切换 = 80秒  45秒&lt;br /&gt;
多工处理    19秒 + 18次工作切换 = 1099秒   20秒 + 19次工作切换 = 1160秒   几近19分钟!!&lt;/p&gt;

&lt;p&gt;工作切换用的时间愈长，多工处理的代价愈大。&lt;/p&gt;

&lt;p&gt;这件事本身不怎么新奇，不是吗？不久大概就会有些白痴气愤地写信指控我「反对」多工处理了。他们会质问我：「你真的想要回到那种得先结束WordPerfect才能执行Lotus 1-2-3的DOS时代吗？」&lt;/p&gt;

&lt;p&gt;不过那并不是我的意思。我只是想要你同意，在这类例子中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;循序处理会让结果平均上比较快得到，而且&lt;/li&gt;
&lt;li&gt;工作切换需要愈久，多工处理所付的代价就愈大。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;够了，别管CPU了，来管管人吧，这有趣多了。这里的重点在于管理「程序员」时，工作切换会需要很长很长的时间。因为程序设计这种工作必须同时在脑袋里记很多东西。另外记住的东西愈多，写程序时生产力愈高。用全速写程序的程序员脑里随时都会记住无数的事情：变量名称，数据结构，重要的API，写过常要用到的辅助函数名称，甚至存放源代码的次目录名称，一切东西都要记住。如果你把程序员送到克利特岛去度假三星期，他所有东西通通都会忘掉。人脑似乎会把东西移出短期RAM，改存到永远都读不回来的备份磁带上。&lt;/p&gt;

&lt;p&gt;要多久呢？嗯，我的软件公司最近放下手头上在做的事(开发一套代号CityDesk的软件产品)，花了三星期去帮助某个客户处理一个紧急状况。当我们回到办公室时，感觉好像要另外三星期才能回复全速制作CityDesk。&lt;/p&gt;

&lt;p&gt;就个人层次来说，你曾经注意过某件事吗？叫某人做一个工作可以做得很好，可是如果给他两个工作，他会把其中一个做好却忽略另一个，不然就是两件工作都做得很慢，慢到你觉得懒鬼都比他勤劳。这是因为程序设计的工作就是需要很长的切换时间。就我自己来说，当我需要同时完成两个程序设计项目时，切换时间大概要六个小时。以一天八小时来看，等于说多工处理把我的生产力降到每天只剩二小时。真令人沮丧啊。&lt;/p&gt;

&lt;p&gt;同样的道理，如果你给某人两件工作，应该要感谢他们只做一件工作而放弃另一件，因为这样能做好更多的事，而且平均上也能更快完成工作。事实上这一切的重点就是绝对不要让人同时做一件以上的事。请确定你有明白它的意思。好的经理人会认为自己的责任是消除障碍，好让大家都能专注在一件事情并把它真的完成。遇到紧急状况时，请先想想能不能自己处理掉，真的不行再丢给深陷在项目中的程序员吧。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>data.table 教程4（校对中）</title>
      <link>http://youngspring1.github.io/post/2016/2016-04-02-datatable4/</link>
      <pubDate>Sat, 02 Apr 2016 07:38:11 +0800</pubDate>
      
      <guid>http://youngspring1.github.io/post/2016/2016-04-02-datatable4/</guid>
      <description>

&lt;p&gt;目录：&lt;br /&gt;
1) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-13-datatable1/&#34;&gt;data.table 介绍&lt;/a&gt;&lt;br /&gt;
2) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-21-datatable2/&#34;&gt;语义引用&lt;/a&gt;&lt;br /&gt;
3) &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-22-datatable3/&#34;&gt;主键、基于快速二分法搜索的subset&lt;/a&gt;&lt;br /&gt;
4) 二级索引和自动索引&lt;br /&gt;
5) Efficient reshaping using data.tables&lt;/p&gt;

&lt;p&gt;原文地址：&lt;br /&gt;
1) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-intro-vignette.html&#34;&gt;Introduction to data.table&lt;/a&gt;&lt;br /&gt;
2) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-reference-semantics.html&#34;&gt;Reference semantics&lt;/a&gt;&lt;br /&gt;
3) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-keys-fast-subset.html&#34;&gt;Keys and fast binary search based subsets&lt;/a&gt;&lt;br /&gt;
4) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-secondary-indices-and-auto-indexing.html&#34;&gt;Secondary indices and auto indexing&lt;/a&gt;&lt;br /&gt;
5) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-reshape.html&#34;&gt;Efficient reshaping using data.tables&lt;/a&gt;&lt;br /&gt;
6) &lt;a href=&#34;https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-faq.html&#34;&gt;Frequently asked questions&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;二级索引和自动索引:3887792cd19298be3383c54afb194a11&#34;&gt;二级索引和自动索引&lt;/h1&gt;

&lt;p&gt;本教程假定读者已经熟悉data.table的[i, j, by]语法、懂得如何基于二分法的选取了。如果你对这些不熟悉，请学习上面三讲 &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-13-datatable1/&#34;&gt;data.table 介绍&lt;/a&gt; 、 &lt;a href=&#34;http://youngspring1.github.io/post/2016-03-21-datatable2/&#34;&gt;语义引用&lt;/a&gt;和&lt;a href=&#34;http://youngspring1.github.io/post/2016-03-22-datatable3/&#34;&gt;主键、基于快速二分法搜索的subset&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;数据:3887792cd19298be3383c54afb194a11&#34;&gt;数据&lt;/h2&gt;

&lt;p&gt;我们继续使用已经保存到本地的航班信息flights。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights &amp;lt;- fread(&amp;quot;flights14.csv&amp;quot;)
head(flights)
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014     1   1        14        13      AA    JFK  LAX      359     2475    9
# 2: 2014     1   1        -3        13      AA    JFK  LAX      363     2475   11
# 3: 2014     1   1         2         9      AA    JFK  LAX      351     2475   19
# 4: 2014     1   1        -8       -26      AA    LGA  PBI      157     1035    7
# 5: 2014     1   1         2         1      AA    JFK  LAX      350     2475   13
# 6: 2014     1   1         4         0      AA    EWR  LAX      339     2454   18
dim(flights)
# [1] 253316     11
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;介绍:3887792cd19298be3383c54afb194a11&#34;&gt;介绍&lt;/h2&gt;

&lt;p&gt;在这一讲，我们会：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;讨论二级索引。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;再次演示快速subset，但这次我们使用新的参数on，它能自动设置二级索引。&lt;/li&gt;
&lt;li&gt;最后进一步的，来看一下自动索引，它也能自动设置索引，但是它是基于R的原生语法来做subset的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-二级索引:3887792cd19298be3383c54afb194a11&#34;&gt;1.二级索引&lt;/h2&gt;

&lt;h4 id=&#34;a-什么是二级索引:3887792cd19298be3383c54afb194a11&#34;&gt;a) 什么是二级索引&lt;/h4&gt;

&lt;p&gt;二级索引和data.table的主键类似，但有以下两点不同：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;它不会再内存里将整个data.table重新排序。它只会计算某列的顺序，将这个顺序向量保存在一个额外的，叫做index的属性里面。&lt;/li&gt;
&lt;li&gt;一个data.table可以有多个二级索引，这是我们下面要演示的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-设置和获取二级索引:3887792cd19298be3383c54afb194a11&#34;&gt;b) 设置和获取二级索引&lt;/h4&gt;

&lt;p&gt;－如何将origin列设置为该data.table的二级索引&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setindex(flights, origin)
head(flights)
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014     1   1        14        13      AA    JFK  LAX      359     2475    9
# 2: 2014     1   1        -3        13      AA    JFK  LAX      363     2475   11
# 3: 2014     1   1         2         9      AA    JFK  LAX      351     2475   19
# 4: 2014     1   1        -8       -26      AA    LGA  PBI      157     1035    7
# 5: 2014     1   1         2         1      AA    JFK  LAX      350     2475   13
# 6: 2014     1   1         4         0      AA    EWR  LAX      339     2454   18

## alternatively we can provide character vectors to the function &#39;setindexv()&#39;
# setindexv(flights, &amp;quot;origin&amp;quot;) # useful to program with

# &#39;index&#39; attribute added
names(attributes(flights))
# [1] &amp;quot;names&amp;quot;             &amp;quot;row.names&amp;quot;         &amp;quot;class&amp;quot;             &amp;quot;.internal.selfref&amp;quot;
# [5] &amp;quot;index&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;函数setindex 和 setindexv()可以对data.table添加一个二级索引。&lt;/li&gt;
&lt;li&gt;注意flights实际上没有按照origin列的升序重新排列。还记得吗？setkey()会重新排序！&lt;/li&gt;
&lt;li&gt;setindex(flights, NULL)会删除所有的二级索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;－如何取得flights的二级索引&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;indices(flights)
# [1] &amp;quot;origin&amp;quot;

setindex(flights, origin, dest)
indices(flights)
# [1] &amp;quot;origin&amp;quot;       &amp;quot;origin__dest&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;函数indices()返回一个data.table所有的二级索引。如果该data.table没有二级索引，那么返回NULL。&lt;/li&gt;
&lt;li&gt;注意我们对 origin列,dest列创建了另一个二级索引的时候，我们没有丢掉之前创建的第一个二级索引。也就是说，我们可以创建多个二级索引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;c-为什么使用二级索引:3887792cd19298be3383c54afb194a11&#34;&gt;c) 为什么使用二级索引&lt;/h4&gt;

&lt;p&gt;－对一个data.table重新排序成本太高
考虑一下这种情况，当你想用主键origin列来subset所有“JFK”的时候，我们得这么做：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## not run
setkey(flights, origin)
flights[&amp;quot;JFK&amp;quot;] # or flights[.(&amp;quot;JFK&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setkey()需要：
a.计算得出origin列的排序向量，并且
b.基于刚刚的排序向量，对整个data.table重新排序
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;排序并不是最花时间的，因为data.table使用对整型、字符型、数值型的向量进行radix排序。然而重新排序却很花时间。&lt;br /&gt;
除非我们需要对某一列重复地进行subset，否则二分法快速subset的高效可能被重新排序抵消。&lt;/p&gt;

&lt;p&gt;－为添加／更新列而对整个data.table重新排序并不理想&lt;br /&gt;
－最多只能有一个主键&lt;br /&gt;
现在我们如果想对dest列是“LAX”的行，重复地进行某个特定的操作，那么我们必须再调用函数setkey() 设置一次主键。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## not run
setkey(flights, dest)
flights[&amp;quot;LAX&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，flights又再次按dest列重新排序了。其实我们真正想做的是，快速地subset同时又不必重新排序。&lt;br /&gt;
这时候，二级索引就派上用场了！&lt;/p&gt;

&lt;p&gt;－二级索引可以被重用&lt;br /&gt;
既然一个data.table中可以有多个二级索引，并且创建一个二级索引就和将一个排序向量保存为属性一样简单，那么创建二级索引后，我们可以省下重新排序的时间。&lt;br /&gt;
－参数on使得语法更简洁，并且能自动创建并重用二级索引&lt;br /&gt;
我们下面一节会说明参数on的几个优点：&lt;/p&gt;

&lt;p&gt;参数on&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过创建索引进行subset。每次都能节省setindex()的时间。&lt;/li&gt;
&lt;li&gt;通过检查属性，可以简单地重用已经存在的二级索引。&lt;/li&gt;
&lt;li&gt;语法简单。
注意参数on也可以用来指定主键。事实上，为了更佳的可读性，我们鼓励在参数on里面指定主键。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-使用参数on和索引进行快速subset:3887792cd19298be3383c54afb194a11&#34;&gt;2.使用参数on和索引进行快速subset&lt;/h2&gt;

&lt;h4 id=&#34;a-参数i里的subset:3887792cd19298be3383c54afb194a11&#34;&gt;a) 参数i里的subset&lt;/h4&gt;

&lt;p&gt;－subset所有origin是“JFK”的行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[&amp;quot;JFK&amp;quot;, on = &amp;quot;origin&amp;quot;]
#        year month day dep_delay arr_delay carrier origin dest air_time distance hour
#     1: 2014     1   1        14        13      AA    JFK  LAX      359     2475    9
#     2: 2014     1   1        -3        13      AA    JFK  LAX      363     2475   11
#     3: 2014     1   1         2         9      AA    JFK  LAX      351     2475   19
#     4: 2014     1   1         2         1      AA    JFK  LAX      350     2475   13
#     5: 2014     1   1        -2       -18      AA    JFK  LAX      338     2475   21
#    ---
# 81479: 2014    10  31        -4       -21      UA    JFK  SFO      337     2586   17
# 81480: 2014    10  31        -2       -37      UA    JFK  SFO      344     2586   18
# 81481: 2014    10  31         0       -33      UA    JFK  LAX      320     2475   17
# 81482: 2014    10  31        -6       -38      UA    JFK  SFO      343     2586    9
# 81483: 2014    10  31        -6       -38      UA    JFK  LAX      323     2475   11

## alternatively
# flights[.(&amp;quot;JFK&amp;quot;), on = &amp;quot;origin&amp;quot;] (or) 
# flights[list(&amp;quot;JFK&amp;quot;), on = &amp;quot;origin&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这段语句执行的subset也是通过创建二级索引，基于快速二分法搜索的。但记住，它不会把这个二级索引自动创建为data.table的一个属性。当然后面我们也会教你如何将它设置为一个属性。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果我们已经添加了一个二级索引了，那么参数on就可以直接使用这个二级索引，而不是再对整个航班信息flights进行计算。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们来看下面 verbose = TRUE 的用法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setindex(flights, origin)
flights[&amp;quot;JFK&amp;quot;, on = &amp;quot;origin&amp;quot;, verbose = TRUE][1:5]
# names(on) = NULL. Assigning &#39;on&#39; to names(on)&#39; as well.
# Looking for existing (secondary) index... found. Reusing index.
# Starting bmerge ...done in 0 secs
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014     1   1        14        13      AA    JFK  LAX      359     2475    9
# 2: 2014     1   1        -3        13      AA    JFK  LAX      363     2475   11
# 3: 2014     1   1         2         9      AA    JFK  LAX      351     2475   19
# 4: 2014     1   1         2         1      AA    JFK  LAX      350     2475   13
# 5: 2014     1   1        -2       -18      AA    JFK  LAX      338     2475   21
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;－如何对origin列和dest列进行subset
举个例子，如果我们想选取所有从“JFK”起飞到达“LAX”的所有航班：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(&amp;quot;JFK&amp;quot;, &amp;quot;LAX&amp;quot;), on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;)][1:5]
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014     1   1        14        13      AA    JFK  LAX      359     2475    9
# 2: 2014     1   1        -3        13      AA    JFK  LAX      363     2475   11
# 3: 2014     1   1         2         9      AA    JFK  LAX      351     2475   19
# 4: 2014     1   1         2         1      AA    JFK  LAX      350     2475   13
# 5: 2014     1   1        -2       -18      AA    JFK  LAX      338     2475   21
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在参数i里面指定取值，在参数on里面指定列名。参数on必须是一个字符型的向量。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;因为计算索引非常快，所以我们不需要使用setindex()。除非你需要对某一列重复地进行subset操作。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-参数j里的select:3887792cd19298be3383c54afb194a11&#34;&gt;b) 参数j里的select&lt;/h4&gt;

&lt;p&gt;下面我们将要讨论的所有操作，跟我们在上一讲里面学习的类似。只是我们现在使用参数on。&lt;br /&gt;
－返回满足条件 origin = &amp;ldquo;LGA&amp;rdquo; and dest = &amp;ldquo;TPA&amp;rdquo;的 arr_delay列的值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(&amp;quot;LGA&amp;quot;, &amp;quot;TPA&amp;quot;), .(arr_delay), on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;)]
#       arr_delay
#    1:         1
#    2:        14
#    3:       -17
#    4:        -4
#    5:       -12
#   ---          
# 1848:        39
# 1849:       -24
# 1850:       -12
# 1851:        21
# 1852:       -11
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;c-chaining:3887792cd19298be3383c54afb194a11&#34;&gt;c) Chaining&lt;/h4&gt;

&lt;p&gt;－在上面的基础上，使用chaining来将结果降序排列&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(&amp;quot;LGA&amp;quot;, &amp;quot;TPA&amp;quot;), .(arr_delay), on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;)][order(-arr_delay)]
#       arr_delay
#    1:       486
#    2:       380
#    3:       351
#    4:       318
#    5:       300
#   ---          
# 1848:       -40
# 1849:       -43
# 1850:       -46
# 1851:       -48
# 1852:       -49
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;d-参数j里的计算:3887792cd19298be3383c54afb194a11&#34;&gt;d) 参数j里的计算&lt;/h4&gt;

&lt;p&gt;－找出满足条件 origin = &amp;ldquo;LGA&amp;rdquo; and dest = &amp;ldquo;TPA&amp;rdquo;的 arr_delay列的最大值（航班到达的最长延误时间）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(&amp;quot;LGA&amp;quot;, &amp;quot;TPA&amp;quot;), max(arr_delay), on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;)]
# [1] 486
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;e-参数j里使用操作符-进行sub-assign:3887792cd19298be3383c54afb194a11&#34;&gt;e) 参数j里使用操作符&amp;rdquo;:=&amp;ldquo;进行sub-assign&lt;/h4&gt;

&lt;p&gt;在上一讲中，我们学习过了类似的功能。同样地，现在我们看看如何找到在flights里面，hours列所有可能的取值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get all &#39;hours&#39; in flights
flights[, sort(unique(hour))]
#  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，一共有25种不同的取值。但是0点和24点其实是同样的。下面我们把24全部替换成0，但是这次我们使用参数on。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(24L), hour := 0L, on = &amp;quot;hour&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们再来看看24是不是都被替换成0了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[, sort(unique(hour))]
#  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这真是二级索引的一大优点。以前，只是为了更新一些行的hour列的取值，我们不得不调用函数setkey()将hour列设置为主键，这必须对整个data.table进行重新排序。但是现在，用参数on，原数据的顺序并没有改变，操作反而更快了！而代码还是如此简洁。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;f-通过参数by聚合:3887792cd19298be3383c54afb194a11&#34;&gt;f) 通过参数by聚合&lt;/h4&gt;

&lt;p&gt;－找到每月从“JFK”起飞的航班起飞的最长延误时间，并按照月份排序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ans &amp;lt;- flights[&amp;quot;JFK&amp;quot;, max(dep_delay), keyby = month, on = &amp;quot;origin&amp;quot;]
head(ans)
#    month   V1
# 1:     1  881
# 2:     2 1014
# 3:     3  920
# 4:     4 1241
# 5:     5  853
# 6:     6  798
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果我们不使用二级索引，也就是不在参数on里面指定，那么我们就必须把origin设置为主键。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;g-参数mult:3887792cd19298be3383c54afb194a11&#34;&gt;g) 参数mult&lt;/h4&gt;

&lt;p&gt;参数mult和上一讲一样。它的默认值是“all”。我们可以选择是第一条还是最后一条符合条件的行被返回。&lt;br /&gt;
－subset满足条件dest ＝ “BOS” 和 “DAY”的第一行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[c(&amp;quot;BOS&amp;quot;, &amp;quot;DAY&amp;quot;), on = &amp;quot;dest&amp;quot;, mult = &amp;quot;first&amp;quot;]
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014     1   1         3         1      AA    JFK  BOS       39      187   12
# 2: 2014     1   1        25        35      EV    EWR  DAY      102      533   17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;－subset满足条件 origin ＝ “LGA” 或者 “JFK” 或者 “EWR”，并且 dest ＝ “XNA” 的最后一行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(c(&amp;quot;LGA&amp;quot;, &amp;quot;JFK&amp;quot;, &amp;quot;EWR&amp;quot;), &amp;quot;XNA&amp;quot;), on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;), mult = &amp;quot;last&amp;quot;]
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014    10  31        -5       -11      MQ    LGA  XNA      165     1147    6
# 2:   NA    NA  NA        NA        NA      NA    JFK  XNA       NA       NA   NA
# 3: 2014    10  31        -2       -25      EV    EWR  XNA      160     1131    6
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;h-参数nomatch:3887792cd19298be3383c54afb194a11&#34;&gt;h) 参数nomatch&lt;/h4&gt;

&lt;p&gt;如果查询语句没有找到任何匹配的数据，通过指定参数nomatch，我们可以选择是返回 NA，还是忽略。&lt;br /&gt;
－在上面这个列子中，忽略没有实际意义的数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flights[.(c(&amp;quot;LGA&amp;quot;, &amp;quot;JFK&amp;quot;, &amp;quot;EWR&amp;quot;), &amp;quot;XNA&amp;quot;), mult = &amp;quot;last&amp;quot;, on = c(&amp;quot;origin&amp;quot;, &amp;quot;dest&amp;quot;), nomatch = 0L]
#    year month day dep_delay arr_delay carrier origin dest air_time distance hour
# 1: 2014    10  31        -5       -11      MQ    LGA  XNA      165     1147    6
# 2: 2014    10  31        -2       -25      EV    EWR  XNA      160     1131    6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;没有航班从“JFK”起飞到达“XNA”，所以结果里面，这一行被忽略了。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-自动索引:3887792cd19298be3383c54afb194a11&#34;&gt;3.自动索引&lt;/h2&gt;

&lt;p&gt;回顾一下，我们先学习如何通过主键使用快速二分法搜索进行subset。接着，我们学习了使用二级索引，它带来更好的效果，而且语法也更简洁。&lt;br /&gt;
等等，有没有更好的方法？有！优化R的原生语法，使用内置的索引。这样我们毋需使用新的语法，就能得到同样的效果。&lt;br /&gt;
这就是自动索引。&lt;br /&gt;
目前，它只支持操作符 == 和 %in% 。而且只对一列起作用。某一列会被自动创建为索引，并且作为data.table的属性保存起来。这跟参数on不同，参数on会每次创建一个临时索引，所以才会被叫做“二级索引”。&lt;/p&gt;

&lt;p&gt;让我们创建一个极大的data.table来凸显它的优势。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set.seed(1L)
dt = data.table(x = sample(1e5L, 1e7L, TRUE), y = runif(100L))
print(object.size(dt), units = &amp;quot;Mb&amp;quot;)
# 114.4 Mb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当我们第一次对某一列使用 == 或者 %in% 的时候，会自动创建一个二级索引，它会被用来进行subset。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# have a look at all the attribute names
names(attributes(dt))
# [1] &amp;quot;names&amp;quot;             &amp;quot;row.names&amp;quot;         &amp;quot;class&amp;quot;             &amp;quot;.internal.selfref&amp;quot;

## run thefirst time
(t1 &amp;lt;- system.time(ans &amp;lt;- dt[x == 989L]))
#    user  system elapsed 
#   0.235   0.013   0.249
head(ans)
#      x         y
# 1: 989 0.5372007
# 2: 989 0.5642786
# 3: 989 0.7151100
# 4: 989 0.3920405
# 5: 989 0.9547465
# 6: 989 0.2914710

## secondary index is created
names(attributes(dt))
# [1] &amp;quot;names&amp;quot;             &amp;quot;row.names&amp;quot;         &amp;quot;class&amp;quot;             &amp;quot;.internal.selfref&amp;quot;
# [5] &amp;quot;index&amp;quot;

indices(dt)
# [1] &amp;quot;x&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一次subset的时候，就是创建索引的时候。因为创建二级索引只会引入一个排序向量，在很多情况下，这种操作符的方式会比扫描向量快得多。所以，从第二次subset开始，自动索引的优势就非常明显了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## successive subsets
(t2 &amp;lt;- system.time(dt[x == 989L]))
#    user  system elapsed 
#   0.001   0.000   0.001
system.time(dt[x %in% 1989:2012])
#    user  system elapsed 
#   0.001   0.000   0.001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一次subset花了0.228秒，但是第二次只花了0.001秒！&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;可以通过设置全局参数关闭自动索引：options(datatable.auto.index = FALSE)。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们正在将二分法搜索扩展到其它的操作符，比如 &amp;lt;, &amp;lt;= 和 &amp;gt;=。完成之后，就能直接用在其他操作符上了。&lt;br /&gt;
在将来，我们计划将自动索引扩展到参数中的其它列。&lt;/p&gt;

&lt;p&gt;在下一讲“结合和滚动结合”中，我们会学习使用主键和二级索引进行快速subset。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>